{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5db0976e-422a-4861-be36-0d77394a2346",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/alexacole/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alexacole/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/alexacole/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "import sklearn\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import scipy.stats as stats\n",
    "#import spacy\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "75b5a047-4688-47da-af77-705569e87c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['source', 'id', 'text'],\n",
       "        num_rows: 1392522\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset('artem9k/ai-text-detection-pile')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7a85aadf-da6e-49da-a4a7-9987e49b454c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>human</td>\n",
       "      <td>0</td>\n",
       "      <td>12 Years a Slave: An Analysis of the Film Essa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>human</td>\n",
       "      <td>1</td>\n",
       "      <td>20+ Social Media Post Ideas to Radically Simpl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>human</td>\n",
       "      <td>2</td>\n",
       "      <td>2022 Russian Invasion of Ukraine in Global Med...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>human</td>\n",
       "      <td>3</td>\n",
       "      <td>533 U.S. 27 (2001) Kyllo v. United States: The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>human</td>\n",
       "      <td>4</td>\n",
       "      <td>A Charles Schwab Corporation Case Essay\\n\\nCha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source  id                                               text\n",
       "0  human   0  12 Years a Slave: An Analysis of the Film Essa...\n",
       "1  human   1  20+ Social Media Post Ideas to Radically Simpl...\n",
       "2  human   2  2022 Russian Invasion of Ukraine in Global Med...\n",
       "3  human   3  533 U.S. 27 (2001) Kyllo v. United States: The...\n",
       "4  human   4  A Charles Schwab Corporation Case Essay\\n\\nCha..."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(dataset['train'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f19051cc-1b32-4871-b433-537c40822f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source\n",
       "human    1028146\n",
       "ai        364376\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.source.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc858b14-9dbe-470f-928f-90881e776f13",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "32b518de-439b-4bcc-93e2-534898f17a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for preprocessing\n",
    "def remove_urls(text):\n",
    "    return re.sub(r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\", \" \", text) # regex taken from https://www.geeksforgeeks.org/python-check-url-string/\n",
    "\n",
    "def remove_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def remove_extra_whitespace(text):\n",
    "    text = text.strip()\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def lemmatizer(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    l = nltk.stem.WordNetLemmatizer()\n",
    "    tokens = [l.lemmatize(token) for token in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def tokenize_pre_process(text): # for preprocessing using this link: https://spotintelligence.com/2022/12/21/nltk-preprocessing-pipeline/\n",
    "    # tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # remove stop words\n",
    "    stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "\n",
    "    # remove top 10% most frequent words \n",
    "    fdist = nltk.FreqDist(tokens)\n",
    "    tokens = [token for token in tokens if fdist[token] < fdist.N() * 0.1]\n",
    "\n",
    "    # stemming\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    # eliminate punctuation\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8857fa40-58c8-4c7b-a125-075104c50009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # encoding to ascii\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    \n",
    "    # convert text to lower case\n",
    "    text = text.lower()\n",
    "\n",
    "    # remove html tags \n",
    "    text = remove_html(text)\n",
    "\n",
    "    # remove urls \n",
    "    text = remove_urls(text)\n",
    "\n",
    "    # remove extra whitespace\n",
    "    text = remove_extra_whitespace(text)\n",
    "\n",
    "    # remove stop words\n",
    "    text = remove_stop_words(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b1afd640-75a0-4bae-95ce-bc0a5a4b8360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text2(text):\n",
    "    # encoding to ascii\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    \n",
    "    # convert text to lower case\n",
    "    text = text.lower()\n",
    "\n",
    "    # remove html tags \n",
    "    text = remove_html(text)\n",
    "\n",
    "    # remove urls \n",
    "    text = remove_urls(text)\n",
    "\n",
    "    # remove extra whitespace\n",
    "    text = remove_extra_whitespace(text)\n",
    "\n",
    "    # remove stop words\n",
    "    text = remove_stop_words(text)\n",
    "\n",
    "    # lemmatize words\n",
    "    text = lemmatizer(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343436b3-c476-4b04-babb-29588e7cb236",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b5910b-2c22-4289-9b51-334e25c0fb26",
   "metadata": {},
   "source": [
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "294c2ed8-1f0f-4bca-ab6e-5392c1e64b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "12b21e42-cb07-4f10-ab2a-c677818219f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(max_df=0.9,min_df=0.1)\n",
    "X = vec.fit_transform(df.text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c571910e-1b05-456e-921f-4b523ecb2fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 615)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "69e51d74-f9c4-47b5-ac77-9995c56eafea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['10', '11', '12', '15', '19', '2017', '2018', '2019', '2020',\n",
       "       '2021', '2022', 'ability', 'able', 'about', 'above', 'access',\n",
       "       'according', 'achieve', 'across', 'act', 'action', 'actions',\n",
       "       'activities', 'activity', 'addition', 'additional', 'additionally',\n",
       "       'address', 'affect', 'affected', 'affects', 'after', 'against',\n",
       "       'age', 'al', 'all', 'allow', 'allowed', 'allows', 'almost',\n",
       "       'already', 'also', 'although', 'always', 'america', 'american',\n",
       "       'americans', 'among', 'an', 'analysis', 'another', 'any',\n",
       "       'approach', 'approaches', 'appropriate', 'are', 'area', 'areas',\n",
       "       'around', 'article', 'aspect', 'aspects', 'associated', 'at',\n",
       "       'attention', 'author', 'available', 'avoid', 'back', 'based',\n",
       "       'basis', 'be', 'became', 'because', 'become', 'becomes', 'been',\n",
       "       'before', 'behavior', 'being', 'believe', 'benefits', 'best',\n",
       "       'better', 'between', 'black', 'body', 'both', 'business', 'but',\n",
       "       'by', 'can', 'cannot', 'care', 'case', 'cases', 'cause', 'caused',\n",
       "       'causes', 'central', 'century', 'certain', 'challenges', 'change',\n",
       "       'changes', 'characteristics', 'children', 'cited', 'citizens',\n",
       "       'clear', 'close', 'come', 'common', 'communication', 'community',\n",
       "       'companies', 'company', 'compared', 'complex', 'concept',\n",
       "       'concerns', 'conclusion', 'condition', 'conditions',\n",
       "       'consequences', 'consider', 'considered', 'considering',\n",
       "       'contents', 'context', 'contribute', 'control', 'cost', 'costs',\n",
       "       'could', 'countries', 'country', 'covid', 'create', 'created',\n",
       "       'creating', 'critical', 'crucial', 'cultural', 'culture',\n",
       "       'current', 'data', 'day', 'decision', 'decisions', 'despite',\n",
       "       'determine', 'develop', 'developed', 'developing', 'development',\n",
       "       'did', 'differences', 'different', 'difficult', 'direct',\n",
       "       'directly', 'discussion', 'disease', 'do', 'does', 'due', 'during',\n",
       "       'each', 'early', 'economic', 'education', 'effect', 'effective',\n",
       "       'effectively', 'effects', 'elements', 'employees', 'end', 'enough',\n",
       "       'ensure', 'environment', 'especially', 'essay', 'essential',\n",
       "       'established', 'et', 'even', 'events', 'every', 'everyone',\n",
       "       'evidence', 'example', 'existing', 'experience', 'face', 'fact',\n",
       "       'factor', 'factors', 'family', 'features', 'feel', 'few', 'field',\n",
       "       'finally', 'financial', 'find', 'first', 'five', 'focus', 'follow',\n",
       "       'following', 'food', 'form', 'forms', 'found', 'framework', 'free',\n",
       "       'from', 'full', 'further', 'furthermore', 'future', 'general',\n",
       "       'get', 'give', 'given', 'global', 'go', 'goal', 'goals', 'good',\n",
       "       'government', 'great', 'group', 'groups', 'growth', 'had', 'hand',\n",
       "       'has', 'have', 'having', 'he', 'health', 'healthcare', 'help',\n",
       "       'helps', 'hence', 'her', 'high', 'higher', 'highly', 'him', 'his',\n",
       "       'history', 'home', 'how', 'however', 'human', 'idea', 'ideas',\n",
       "       'identify', 'if', 'impact', 'importance', 'important', 'improve',\n",
       "       'include', 'includes', 'including', 'increase', 'increased',\n",
       "       'increasing', 'individual', 'individuals', 'industry', 'influence',\n",
       "       'information', 'instance', 'instead', 'interest', 'international',\n",
       "       'into', 'introduction', 'involved', 'issue', 'issues', 'its',\n",
       "       'itself', 'journal', 'just', 'key', 'know', 'knowledge', 'known',\n",
       "       'lack', 'large', 'last', 'later', 'law', 'lead', 'leading',\n",
       "       'leads', 'learning', 'led', 'legal', 'less', 'level', 'levels',\n",
       "       'life', 'like', 'likely', 'limited', 'live', 'lives', 'living',\n",
       "       'local', 'long', 'low', 'lower', 'made', 'main', 'maintain',\n",
       "       'major', 'make', 'makes', 'making', 'man', 'management', 'many',\n",
       "       'market', 'matter', 'may', 'me', 'meaning', 'means', 'measures',\n",
       "       'media', 'medical', 'members', 'mental', 'mentioned', 'method',\n",
       "       'methods', 'might', 'model', 'modern', 'money', 'more', 'moreover',\n",
       "       'most', 'much', 'multiple', 'must', 'my', 'national', 'natural',\n",
       "       'nature', 'necessary', 'need', 'needed', 'needs', 'negative',\n",
       "       'never', 'new', 'next', 'no', 'non', 'not', 'now', 'number',\n",
       "       'numerous', 'often', 'one', 'ones', 'only', 'operations',\n",
       "       'opinion', 'opportunities', 'opportunity', 'or', 'order',\n",
       "       'organization', 'organizations', 'other', 'others', 'our', 'out',\n",
       "       'outcomes', 'over', 'overall', 'own', 'pandemic', 'paper', 'part',\n",
       "       'particular', 'particularly', 'past', 'patient', 'patients',\n",
       "       'people', 'performance', 'period', 'person', 'personal',\n",
       "       'perspective', 'physical', 'place', 'plan', 'play', 'point',\n",
       "       'policy', 'political', 'population', 'position', 'positive',\n",
       "       'possible', 'potential', 'power', 'pp', 'practice', 'practices',\n",
       "       'present', 'presented', 'prevent', 'primary', 'principles',\n",
       "       'problem', 'problems', 'process', 'processes', 'product',\n",
       "       'production', 'products', 'professional', 'promote', 'proper',\n",
       "       'provide', 'provided', 'provides', 'providing', 'public',\n",
       "       'purpose', 'quality', 'question', 'range', 'rate', 'rather',\n",
       "       'real', 'reason', 'reasons', 'receive', 'reduce', 'reference',\n",
       "       'references', 'regarding', 'related', 'relationship',\n",
       "       'relationships', 'relevant', 'report', 'require', 'required',\n",
       "       'requires', 'research', 'resources', 'responsibility',\n",
       "       'responsible', 'result', 'results', 'review', 'right', 'rights',\n",
       "       'risk', 'risks', 'role', 'safety', 'same', 'science', 'second',\n",
       "       'security', 'see', 'seen', 'self', 'service', 'services', 'set',\n",
       "       'several', 'severe', 'share', 'she', 'should', 'show', 'shows',\n",
       "       'significant', 'significantly', 'similar', 'since', 'situation',\n",
       "       'skills', 'small', 'so', 'social', 'society', 'some', 'source',\n",
       "       'sources', 'specific', 'spread', 'state', 'states', 'status',\n",
       "       'still', 'story', 'strategies', 'strategy', 'strong', 'structure',\n",
       "       'studies', 'study', 'subject', 'success', 'successful', 'such',\n",
       "       'support', 'system', 'systems', 'table', 'take', 'taken', 'taking',\n",
       "       'technology', 'term', 'terms', 'than', 'their', 'them',\n",
       "       'themselves', 'then', 'theory', 'there', 'therefore', 'these',\n",
       "       'they', 'third', 'this', 'those', 'though', 'three', 'through',\n",
       "       'throughout', 'thus', 'time', 'times', 'today', 'together',\n",
       "       'topic', 'towards', 'treatment', 'turn', 'two', 'type', 'types',\n",
       "       'under', 'understand', 'understanding', 'unique', 'united',\n",
       "       'university', 'up', 'us', 'use', 'used', 'uses', 'using',\n",
       "       'usually', 'value', 'values', 'various', 'very', 'view', 'vital',\n",
       "       'want', 'was', 'way', 'ways', 'we', 'web', 'well', 'were', 'what',\n",
       "       'when', 'where', 'whether', 'which', 'while', 'white', 'who',\n",
       "       'whole', 'why', 'will', 'within', 'without', 'women', 'words',\n",
       "       'work', 'workers', 'working', 'works', 'world', 'would', 'year',\n",
       "       'years', 'you', 'young'], dtype=object)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2f22d6b9-f08c-4ac8-a8fa-6598f213a5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec2 = CountVectorizer(preprocessor=preprocess_text,max_df=0.9,min_df=0.1)\n",
    "X2 = vec2.fit_transform(df.text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "be6ff840-a8e5-4cfe-86ec-2eefd27edc34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['10', '11', '12', '15', '19', '2017', '2018', '2019', '2020',\n",
       "       '2021', '2022', 'ability', 'able', 'access', 'according',\n",
       "       'achieve', 'across', 'act', 'action', 'actions', 'activities',\n",
       "       'activity', 'addition', 'additional', 'additionally', 'address',\n",
       "       'affect', 'affected', 'affects', 'age', 'al', 'allow', 'allowed',\n",
       "       'allows', 'almost', 'already', 'also', 'although', 'always',\n",
       "       'america', 'american', 'americans', 'among', 'analysis', 'another',\n",
       "       'approach', 'approaches', 'appropriate', 'area', 'areas', 'around',\n",
       "       'article', 'aspect', 'aspects', 'associated', 'attention',\n",
       "       'author', 'authors', 'available', 'avoid', 'back', 'based',\n",
       "       'basis', 'became', 'become', 'becomes', 'behavior', 'believe',\n",
       "       'benefits', 'best', 'better', 'body', 'business', 'care', 'case',\n",
       "       'cases', 'cause', 'caused', 'causes', 'central', 'century',\n",
       "       'certain', 'challenges', 'change', 'changes', 'characteristics',\n",
       "       'children', 'cited', 'citizens', 'clear', 'close', 'come',\n",
       "       'common', 'communication', 'community', 'companies', 'company',\n",
       "       'companys', 'compared', 'complex', 'concept', 'concerns',\n",
       "       'conclusion', 'condition', 'conditions', 'consequences',\n",
       "       'consider', 'considered', 'considering', 'contents', 'context',\n",
       "       'contribute', 'control', 'cost', 'costs', 'could', 'countries',\n",
       "       'country', 'covid', 'create', 'created', 'creating', 'critical',\n",
       "       'crucial', 'cultural', 'culture', 'current', 'data', 'day',\n",
       "       'decision', 'decisions', 'despite', 'determine', 'develop',\n",
       "       'developed', 'developing', 'development', 'differences',\n",
       "       'different', 'difficult', 'direct', 'directly', 'discussion',\n",
       "       'disease', 'due', 'early', 'economic', 'education', 'effect',\n",
       "       'effective', 'effectively', 'effects', 'elements', 'employees',\n",
       "       'end', 'enough', 'ensure', 'environment', 'especially', 'essay',\n",
       "       'essential', 'established', 'et', 'even', 'events', 'every',\n",
       "       'evidence', 'example', 'existing', 'experience', 'face', 'fact',\n",
       "       'factor', 'factors', 'family', 'features', 'feel', 'field',\n",
       "       'finally', 'financial', 'find', 'first', 'five', 'focus', 'follow',\n",
       "       'following', 'food', 'form', 'forms', 'found', 'framework', 'free',\n",
       "       'full', 'furthermore', 'future', 'general', 'get', 'give', 'given',\n",
       "       'global', 'go', 'goal', 'goals', 'good', 'government', 'great',\n",
       "       'group', 'groups', 'growth', 'hand', 'health', 'healthcare',\n",
       "       'help', 'helps', 'hence', 'high', 'higher', 'highly', 'history',\n",
       "       'home', 'however', 'human', 'idea', 'ideas', 'identify', 'impact',\n",
       "       'importance', 'important', 'improve', 'include', 'includes',\n",
       "       'including', 'increase', 'increased', 'increasing', 'individual',\n",
       "       'individuals', 'industry', 'influence', 'information', 'instance',\n",
       "       'instead', 'interest', 'international', 'introduction', 'involved',\n",
       "       'issue', 'issues', 'journal', 'key', 'know', 'knowledge', 'known',\n",
       "       'lack', 'large', 'last', 'later', 'law', 'lead', 'leading',\n",
       "       'leads', 'learning', 'led', 'legal', 'less', 'level', 'levels',\n",
       "       'life', 'like', 'likely', 'limited', 'live', 'lives', 'living',\n",
       "       'local', 'long', 'low', 'lower', 'made', 'main', 'maintain',\n",
       "       'major', 'make', 'makes', 'making', 'management', 'many', 'market',\n",
       "       'matter', 'may', 'meaning', 'means', 'measures', 'media',\n",
       "       'medical', 'members', 'mental', 'mentioned', 'method', 'methods',\n",
       "       'might', 'model', 'modern', 'money', 'moreover', 'much',\n",
       "       'multiple', 'must', 'national', 'natural', 'nature', 'necessary',\n",
       "       'need', 'needed', 'needs', 'negative', 'never', 'new', 'next',\n",
       "       'non', 'number', 'numerous', 'often', 'one', 'ones', 'operations',\n",
       "       'opinion', 'opportunities', 'opportunity', 'order', 'organization',\n",
       "       'organizations', 'others', 'outcomes', 'overall', 'pandemic',\n",
       "       'paper', 'part', 'particular', 'particularly', 'past', 'patient',\n",
       "       'patients', 'people', 'peoples', 'performance', 'period', 'person',\n",
       "       'personal', 'persons', 'perspective', 'physical', 'place', 'plan',\n",
       "       'play', 'point', 'policy', 'political', 'population', 'position',\n",
       "       'positive', 'possible', 'potential', 'power', 'pp', 'practice',\n",
       "       'practices', 'present', 'presented', 'prevent', 'primary',\n",
       "       'principles', 'problem', 'problems', 'process', 'processes',\n",
       "       'product', 'production', 'products', 'professional', 'programs',\n",
       "       'promote', 'proper', 'provide', 'provided', 'provides',\n",
       "       'providing', 'public', 'purpose', 'quality', 'question', 'range',\n",
       "       'rate', 'rather', 'real', 'reason', 'reasons', 'receive', 'reduce',\n",
       "       'reference', 'references', 'regarding', 'related', 'relationship',\n",
       "       'relationships', 'relevant', 'report', 'require', 'required',\n",
       "       'requires', 'research', 'resources', 'responsibility',\n",
       "       'responsible', 'result', 'results', 'review', 'right', 'rights',\n",
       "       'risk', 'risks', 'role', 'safety', 'science', 'second', 'security',\n",
       "       'see', 'seen', 'self', 'service', 'services', 'set', 'several',\n",
       "       'severe', 'share', 'show', 'shows', 'significant', 'significantly',\n",
       "       'similar', 'since', 'situation', 'skills', 'small', 'social',\n",
       "       'society', 'source', 'sources', 'specific', 'spread', 'state',\n",
       "       'states', 'status', 'still', 'strategies', 'strategy', 'strong',\n",
       "       'structure', 'studies', 'study', 'subject', 'success',\n",
       "       'successful', 'support', 'system', 'systems', 'table', 'take',\n",
       "       'taken', 'taking', 'technology', 'term', 'terms', 'theory',\n",
       "       'therefore', 'third', 'though', 'three', 'throughout', 'thus',\n",
       "       'time', 'times', 'together', 'topic', 'towards', 'treatment',\n",
       "       'turn', 'two', 'type', 'types', 'understand', 'understanding',\n",
       "       'unique', 'united', 'university', 'us', 'use', 'used', 'uses',\n",
       "       'using', 'usually', 'value', 'values', 'various', 'view', 'vital',\n",
       "       'want', 'way', 'ways', 'web', 'well', 'whether', 'white', 'whole',\n",
       "       'within', 'without', 'women', 'words', 'work', 'workers',\n",
       "       'working', 'works', 'world', 'would', 'year', 'years', 'young'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec2.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "babe2fe5-7e00-49a3-928d-4dae81182844",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec3 = CountVectorizer(preprocessor=preprocess_text2,max_df=0.9,min_df=0.1)\n",
    "X3 = vec3.fit_transform(df.text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4165a5c5-cea7-42fc-9d2a-fd5f1e413640",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['10', '11', '12', '15', '19', '2017', '2018', '2019', '2020',\n",
       "       '2021', '2022', 'ability', 'able', 'access', 'according',\n",
       "       'account', 'achieve', 'across', 'act', 'action', 'activity',\n",
       "       'addition', 'additional', 'additionally', 'address', 'advantage',\n",
       "       'affect', 'affected', 'age', 'aim', 'al', 'allow', 'allowed',\n",
       "       'allows', 'almost', 'already', 'also', 'although', 'always',\n",
       "       'america', 'american', 'among', 'amount', 'analysis', 'another',\n",
       "       'application', 'approach', 'appropriate', 'area', 'around',\n",
       "       'article', 'aspect', 'assessment', 'associated', 'attention',\n",
       "       'attitude', 'author', 'authority', 'available', 'avoid', 'back',\n",
       "       'background', 'based', 'basis', 'became', 'become', 'becomes',\n",
       "       'behavior', 'being', 'belief', 'believe', 'benefit', 'best',\n",
       "       'better', 'black', 'body', 'book', 'business', 'care', 'case',\n",
       "       'cause', 'caused', 'center', 'central', 'century', 'certain',\n",
       "       'challenge', 'chance', 'change', 'character', 'characteristic',\n",
       "       'child', 'choice', 'cited', 'citizen', 'claim', 'clear', 'close',\n",
       "       'come', 'common', 'communication', 'community', 'company',\n",
       "       'compared', 'complex', 'component', 'concept', 'concern',\n",
       "       'conclusion', 'condition', 'conduct', 'conflict', 'connection',\n",
       "       'consequence', 'consider', 'consideration', 'considered',\n",
       "       'considering', 'content', 'context', 'contribute', 'control',\n",
       "       'cost', 'could', 'country', 'covid', 'create', 'created',\n",
       "       'creating', 'critical', 'crucial', 'cultural', 'culture',\n",
       "       'current', 'customer', 'data', 'day', 'death', 'decision',\n",
       "       'demand', 'desire', 'despite', 'detail', 'determine', 'develop',\n",
       "       'developed', 'developing', 'development', 'difference',\n",
       "       'different', 'difficult', 'direct', 'directly', 'discussion',\n",
       "       'disease', 'due', 'early', 'economic', 'ed', 'education', 'effect',\n",
       "       'effective', 'effectively', 'effort', 'element', 'employee', 'end',\n",
       "       'enough', 'ensure', 'environment', 'especially', 'essay',\n",
       "       'essential', 'established', 'et', 'even', 'event', 'every',\n",
       "       'evidence', 'example', 'existing', 'experience', 'face', 'fact',\n",
       "       'factor', 'family', 'feature', 'feel', 'feeling', 'field',\n",
       "       'finally', 'financial', 'find', 'finding', 'first', 'five',\n",
       "       'focus', 'follow', 'following', 'food', 'force', 'form', 'found',\n",
       "       'framework', 'free', 'full', 'function', 'furthermore', 'future',\n",
       "       'general', 'get', 'give', 'given', 'global', 'go', 'goal', 'good',\n",
       "       'government', 'great', 'group', 'growth', 'hand', 'health',\n",
       "       'healthcare', 'help', 'hence', 'high', 'higher', 'highly',\n",
       "       'history', 'home', 'however', 'human', 'idea', 'identify', 'image',\n",
       "       'impact', 'importance', 'important', 'improve', 'improvement',\n",
       "       'include', 'includes', 'including', 'income', 'increase',\n",
       "       'increased', 'increasing', 'individual', 'industry', 'influence',\n",
       "       'information', 'instance', 'instead', 'interaction', 'interest',\n",
       "       'international', 'introduction', 'involved', 'issue', 'job',\n",
       "       'journal', 'keep', 'key', 'know', 'knowledge', 'known', 'lack',\n",
       "       'large', 'last', 'later', 'law', 'lead', 'leader', 'leading',\n",
       "       'learning', 'led', 'legal', 'less', 'level', 'life', 'like',\n",
       "       'likely', 'limited', 'live', 'living', 'local', 'long', 'look',\n",
       "       'loss', 'low', 'lower', 'made', 'main', 'maintain', 'major',\n",
       "       'make', 'making', 'man', 'management', 'many', 'market',\n",
       "       'material', 'matter', 'may', 'mean', 'meaning', 'measure',\n",
       "       'medical', 'medicine', 'medium', 'meet', 'member', 'mental',\n",
       "       'mentioned', 'method', 'might', 'million', 'model', 'modern',\n",
       "       'money', 'moreover', 'movement', 'much', 'multiple', 'must',\n",
       "       'national', 'natural', 'nature', 'necessary', 'need', 'needed',\n",
       "       'negative', 'never', 'new', 'next', 'non', 'note', 'number',\n",
       "       'numerous', 'objective', 'offer', 'often', 'one', 'open',\n",
       "       'operation', 'opinion', 'opportunity', 'order', 'organization',\n",
       "       'others', 'outcome', 'overall', 'pandemic', 'paper', 'part',\n",
       "       'particular', 'particularly', 'past', 'patient', 'pay', 'people',\n",
       "       'perception', 'performance', 'period', 'person', 'personal',\n",
       "       'perspective', 'physical', 'place', 'plan', 'play', 'point',\n",
       "       'policy', 'political', 'population', 'position', 'positive',\n",
       "       'possibility', 'possible', 'potential', 'power', 'pp', 'practice',\n",
       "       'present', 'presented', 'press', 'prevent', 'primary', 'principle',\n",
       "       'problem', 'process', 'product', 'production', 'professional',\n",
       "       'program', 'project', 'promote', 'proper', 'provide', 'provided',\n",
       "       'provides', 'providing', 'public', 'purpose', 'put', 'quality',\n",
       "       'question', 'range', 'rate', 'rather', 'real', 'reason', 'receive',\n",
       "       'reduce', 'reference', 'regarding', 'related', 'relation',\n",
       "       'relationship', 'relevant', 'report', 'require', 'required',\n",
       "       'requires', 'research', 'researcher', 'resource', 'response',\n",
       "       'responsibility', 'responsible', 'result', 'review', 'right',\n",
       "       'risk', 'role', 'rule', 'safety', 'say', 'science', 'second',\n",
       "       'security', 'see', 'seen', 'self', 'service', 'set', 'setting',\n",
       "       'several', 'severe', 'share', 'show', 'side', 'significant',\n",
       "       'significantly', 'similar', 'since', 'situation', 'skill', 'small',\n",
       "       'social', 'society', 'solution', 'source', 'specific', 'spread',\n",
       "       'stage', 'standard', 'start', 'state', 'statement', 'status',\n",
       "       'step', 'still', 'story', 'strategy', 'strong', 'structure',\n",
       "       'study', 'subject', 'success', 'successful', 'support', 'system',\n",
       "       'table', 'take', 'taken', 'taking', 'task', 'technology', 'term',\n",
       "       'theory', 'therefore', 'thing', 'think', 'third', 'though',\n",
       "       'thought', 'threat', 'three', 'throughout', 'thus', 'time',\n",
       "       'today', 'together', 'tool', 'topic', 'towards', 'treatment',\n",
       "       'trend', 'turn', 'two', 'type', 'understand', 'understanding',\n",
       "       'unique', 'united', 'university', 'us', 'use', 'used', 'using',\n",
       "       'usually', 'value', 'various', 'view', 'vital', 'want', 'way',\n",
       "       'web', 'well', 'whether', 'white', 'whole', 'within', 'without',\n",
       "       'woman', 'word', 'work', 'worker', 'working', 'world', 'would',\n",
       "       'writing', 'year', 'young'], dtype=object)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec3.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "368a78d5-0e20-4500-8d23-7bac8accebf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['al 2020', 'content introduction', 'covid 19', 'essay table',\n",
       "       'essay table content', 'et al', 'et al 2020', 'research paper',\n",
       "       'table content', 'table content introduction', 'united state',\n",
       "       'work cited'], dtype=object)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec4 = CountVectorizer(preprocessor=preprocess_text2,max_df=0.9,min_df=0.1, ngram_range=(2,3))\n",
    "X4 = vec4.fit_transform(df.text[:1000])\n",
    "vec4.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cd1285-0e3e-4274-8163-dd0425dfb588",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5cb2a20f-7761-417c-a22b-0c53a82c8358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(data_train, data_test):\n",
    "    tfidf = TfidfVectorizer(preprocessor=preprocess_text2,max_df=0.9,min_df=0.1)\n",
    "    train = tfidf.fit_transform(data_train)\n",
    "    test = tfidf.transform(data_test)\n",
    "    return train, test "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ad9fab-ff72-4a2f-a7f8-e44bf51f062e",
   "metadata": {},
   "source": [
    "### Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6b505b72-5113-4ebc-945d-17283978efdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "76b00d20-428b-47a5-8b29-82a20c4d17c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc2vec(data):\n",
    "    '''\n",
    "    https://www.geeksforgeeks.org/doc2vec-in-nlp/\n",
    "    '''\n",
    "    \n",
    "    tagged_data = [TaggedDocument(words=word_tokenize(doc.lower()),\n",
    "                              tags=[str(i)]) for i,doc in enumerate(data)]\n",
    "    # train the Doc2vec model\n",
    "    model = Doc2Vec(vector_size=20,\n",
    "                    min_count=2, epochs=50)\n",
    "    model.build_vocab(tagged_data)\n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.epochs)\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_vectors(doc2vec_model, data):\n",
    "    # get the document vectors\n",
    "    return [doc2vec_model.infer_vector(word_tokenize(doc.lower())) for doc in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a7562a-3e07-4f15-ab70-5228896fe154",
   "metadata": {},
   "source": [
    "## Sampling from the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810c3b8a-4af6-48ac-ab6d-8765ecff56e9",
   "metadata": {},
   "source": [
    "### Preserving the original proportions of humans and ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8dc50736-666f-46f5-b7f1-8e4a03410827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent ai: 35.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q7/lxfgdwq942qdsd3_mlwlm7p00000gn/T/ipykernel_62893/875810769.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  data = df.groupby('source', group_keys=False).apply(lambda x: x.sample(frac=0.001, random_state=0))\n"
     ]
    }
   ],
   "source": [
    "# Reporting the proportion of samples that are ai generated\n",
    "print('Percent ai:', round(df[df['source'] == 'ai'].shape[0]/df[df['source'] == 'human'].shape[0]*100, 3))\n",
    "\n",
    "# Taking a stratified sample of 0.1% of the data\n",
    "# maintaining same proportions of human and ai samples\n",
    "data = df.groupby('source', group_keys=False).apply(lambda x: x.sample(frac=0.001, random_state=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60b2f4c-a92b-4247-bc9f-3c9120185e6e",
   "metadata": {},
   "source": [
    "### Randomly sampling 2000 samples each of human and ai sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "526cb205-3d63-41d8-85fd-b056e24105c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>human</td>\n",
       "      <td>198049</td>\n",
       "      <td>Overview\\n\\nBatman and Psychology: A Dark and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>human</td>\n",
       "      <td>12919</td>\n",
       "      <td>The Use of Psychedelic Drugs in Treating Depre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>human</td>\n",
       "      <td>979845</td>\n",
       "      <td>member the day like it was yesterday. \\n My mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>human</td>\n",
       "      <td>73499</td>\n",
       "      <td>Legislative Branch Power, Its Limits and Expan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>human</td>\n",
       "      <td>44380</td>\n",
       "      <td>Growth and Fall of Vader Corporation Report\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>ai</td>\n",
       "      <td>1052501</td>\n",
       "      <td>\"Learn Python the Hard Way\" by Zed Shaw\\n\\t\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>ai</td>\n",
       "      <td>1277472</td>\n",
       "      <td>Cape Town - The South African Humanist Associa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>ai</td>\n",
       "      <td>1086015</td>\n",
       "      <td>We are only days away from the 2016 NFL Draft,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>ai</td>\n",
       "      <td>1338694</td>\n",
       "      <td>The number of people infected with Zika virus ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>ai</td>\n",
       "      <td>1189675</td>\n",
       "      <td>Dorothy Lee Hough - May 24, 2017\\n\\nDorothy Le...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     source       id                                               text\n",
       "0     human   198049  Overview\\n\\nBatman and Psychology: A Dark and ...\n",
       "1     human    12919  The Use of Psychedelic Drugs in Treating Depre...\n",
       "2     human   979845  member the day like it was yesterday. \\n My mo...\n",
       "3     human    73499  Legislative Branch Power, Its Limits and Expan...\n",
       "4     human    44380  Growth and Fall of Vader Corporation Report\\n\\...\n",
       "...     ...      ...                                                ...\n",
       "3995     ai  1052501   \"Learn Python the Hard Way\" by Zed Shaw\\n\\t\\t...\n",
       "3996     ai  1277472  Cape Town - The South African Humanist Associa...\n",
       "3997     ai  1086015  We are only days away from the 2016 NFL Draft,...\n",
       "3998     ai  1338694  The number of people infected with Zika virus ...\n",
       "3999     ai  1189675  Dorothy Lee Hough - May 24, 2017\\n\\nDorothy Le...\n",
       "\n",
       "[4000 rows x 3 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly sampling 2000 human and 2000 ai samples to create dataset with equal proportions\n",
    "human = df[df.source == \"human\"]\n",
    "human = human.sample(2000, random_state=0)\n",
    "ai = df[df.source == \"ai\"].sample(2000, random_state=0)\n",
    "equal = pd.concat([human,ai], ignore_index=True)\n",
    "equal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba83c9c4-2c3d-4634-85dc-74cb65e4c657",
   "metadata": {},
   "source": [
    "# Run Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5b0ab80d-c08f-492f-bf2a-751acb2454ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CI(metric, confidence):\n",
    "    a,b = stats.t.interval(confidence, \n",
    "                         len(metric)-1, \n",
    "                         loc=metric.mean(), \n",
    "                         scale=metric.std(ddof=1)/np.sqrt(len(metric)))\n",
    "    return a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0c739f1d-53ae-4c46-b02b-db9d32b0ab08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(X_train, X_test, t_train, t_test, model, model_name, confidence=0.95, scoring='accuracy'):\n",
    "\n",
    "    y_train = model.predict(X_train)\n",
    "    y_test = model.predict(X_test)\n",
    "    \n",
    "    scores = cross_val_score(model,\n",
    "                             X_train, \n",
    "                             t_train, \n",
    "                             scoring=scoring, \n",
    "                             cv=KFold(10, shuffle=True, random_state=0))\n",
    "    \n",
    "    a,b = CI(scores, confidence)\n",
    "    \n",
    "    print(f'==================={model_name} Performance=====================')\n",
    "    print('95% CI = [', a, b, ']')\n",
    "    print('Train: ', classification_report(t_train, y_train))\n",
    "    print('Test: ', classification_report(t_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "16c36ae6-8b8a-4769-94c6-6483546da92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_models(data, features):\n",
    "    # Convert labels into numeric\n",
    "    t = data.source\n",
    "    d = {'human' : 0, 'ai' : 1}\n",
    "    t = t.map(d, na_action='ignore')\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, t_train, t_test = train_test_split(np.array(data.text), t, test_size=0.2, random_state=0)\n",
    "    \n",
    "    if features == 'tfidf':\n",
    "        X_train, X_test = tfidf(X_train,X_test)\n",
    "        X_train = X_train.toarray()\n",
    "        X_test = X_test.toarray()\n",
    "    elif features == 'doc2vec':\n",
    "        train_prep = []\n",
    "        test_prep = []\n",
    "        for i in range(len(X_train)):\n",
    "            train_prep += [preprocess_text2(X_train[i])]\n",
    "        for i in range(len(X_test)):\n",
    "            test_prep += [preprocess_text2(X_test[i])]\n",
    "        \n",
    "        m = doc2vec(train_prep)\n",
    "        X_train = np.array(get_vectors(m, train_prep))\n",
    "        X_test = np.array(get_vectors(m, test_prep))\n",
    "\n",
    "    # Naive Bayes -- need to scale for doc2vec\n",
    "    if features == 'tfidf':\n",
    "        mnb_pipe = Pipeline([('mnb', MultinomialNB())])\n",
    "    elif features == 'doc2vec':\n",
    "        mnb_pipe = Pipeline([('scaler', MinMaxScaler()),('mnb', MultinomialNB())])\n",
    "    mnb_param = {'mnb__alpha': [0.2, 0.4, 0.6, 0.8, 1], 'mnb__force_alpha': [True, False]}\n",
    "    \n",
    "    \n",
    "    gs1 = GridSearchCV(mnb_pipe, \n",
    "                   param_grid=mnb_param,\n",
    "                   cv=KFold(n_splits=5, shuffle=True, random_state=0), \n",
    "                   scoring='accuracy',\n",
    "                   verbose=1, \n",
    "                   n_jobs=-1, \n",
    "                   refit=True)\n",
    "\n",
    "    gs1.fit(X_train, t_train)\n",
    "    mnb = gs1.best_estimator_\n",
    "    evaluation(X_train, X_test, t_train, t_test, mnb, 'MNB')\n",
    "    \n",
    "    # Decision Tree\n",
    "    dt_pipe = Pipeline([('dt', DecisionTreeClassifier())])\n",
    "    # hyperparameters selected taken from https://www.geeksforgeeks.org/how-to-tune-a-decision-tree-in-hyperparameter-tuning/\n",
    "    dt_param = {'dt__max_depth': [10, 20, 30, None],'dt__min_samples_split': [2, 5, 10],'dt__min_samples_leaf': [1, 2, 4]}\n",
    "    \n",
    "    gs2 = GridSearchCV(dt_pipe, \n",
    "                   param_grid=dt_param,\n",
    "                   cv=KFold(n_splits=5, shuffle=True, random_state=0), \n",
    "                   scoring='accuracy',\n",
    "                   verbose=1, \n",
    "                   n_jobs=-1, \n",
    "                   refit=True)\n",
    "\n",
    "    gs2.fit(X_train, t_train)\n",
    "    dt = gs2.best_estimator_\n",
    "    evaluation(X_train, X_test, t_train, t_test, dt, 'DT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3dc7dc79-d3ca-4d5e-a0a8-c01ab0c5442c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q7/lxfgdwq942qdsd3_mlwlm7p00000gn/T/ipykernel_62893/3287934322.py:6: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "===================MNB Performance=====================\n",
      "95% CI = [ 0.7184251008082508 0.7675626726795227 ]\n",
      "Train:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      1.00      0.85       827\n",
      "           1       0.00      0.00      0.00       286\n",
      "\n",
      "    accuracy                           0.74      1113\n",
      "   macro avg       0.37      0.50      0.43      1113\n",
      "weighted avg       0.55      0.74      0.63      1113\n",
      "\n",
      "Test:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      1.00      0.84       201\n",
      "           1       0.00      0.00      0.00        78\n",
      "\n",
      "    accuracy                           0.72       279\n",
      "   macro avg       0.36      0.50      0.42       279\n",
      "weighted avg       0.52      0.72      0.60       279\n",
      "\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/PR_Project/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/envs/PR_Project/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/envs/PR_Project/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/envs/PR_Project/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/envs/PR_Project/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/envs/PR_Project/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================DT Performance=====================\n",
      "95% CI = [ 0.6804533049591012 0.7625003629945669 ]\n",
      "Train:                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       827\n",
      "           1       1.00      1.00      1.00       286\n",
      "\n",
      "    accuracy                           1.00      1113\n",
      "   macro avg       1.00      1.00      1.00      1113\n",
      "weighted avg       1.00      1.00      1.00      1113\n",
      "\n",
      "Test:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.85      0.83       201\n",
      "           1       0.57      0.50      0.53        78\n",
      "\n",
      "    accuracy                           0.75       279\n",
      "   macro avg       0.69      0.68      0.68       279\n",
      "weighted avg       0.74      0.75      0.75       279\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_models(data, 'doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "189e3dc0-f974-4f89-aff5-1193865f830a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q7/lxfgdwq942qdsd3_mlwlm7p00000gn/T/ipykernel_62893/3287934322.py:6: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "===================MNB Performance=====================\n",
      "95% CI = [ 0.7228832636808271 0.7738027080051444 ]\n",
      "Train:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.98      0.86       827\n",
      "           1       0.65      0.12      0.20       286\n",
      "\n",
      "    accuracy                           0.76      1113\n",
      "   macro avg       0.71      0.55      0.53      1113\n",
      "weighted avg       0.73      0.76      0.69      1113\n",
      "\n",
      "Test:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.98      0.84       201\n",
      "           1       0.56      0.06      0.11        78\n",
      "\n",
      "    accuracy                           0.72       279\n",
      "   macro avg       0.64      0.52      0.48       279\n",
      "weighted avg       0.68      0.72      0.63       279\n",
      "\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "===================DT Performance=====================\n",
      "95% CI = [ 0.7196855791542295 0.7644199549513045 ]\n",
      "Train:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.95      0.92       827\n",
      "           1       0.82      0.62      0.71       286\n",
      "\n",
      "    accuracy                           0.87      1113\n",
      "   macro avg       0.85      0.79      0.81      1113\n",
      "weighted avg       0.87      0.87      0.86      1113\n",
      "\n",
      "Test:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.86      0.82       201\n",
      "           1       0.54      0.44      0.48        78\n",
      "\n",
      "    accuracy                           0.74       279\n",
      "   macro avg       0.67      0.65      0.65       279\n",
      "weighted avg       0.72      0.74      0.73       279\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_models(data, 'tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0432b251-7ea7-4532-8ab4-49d3fe8ad519",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q7/lxfgdwq942qdsd3_mlwlm7p00000gn/T/ipykernel_62893/3287934322.py:6: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "===================MNB Performance=====================\n",
      "95% CI = [ 0.6635121697952623 0.7527378302047375 ]\n",
      "Train:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.67      0.74      1593\n",
      "           1       0.72      0.85      0.78      1607\n",
      "\n",
      "    accuracy                           0.76      3200\n",
      "   macro avg       0.77      0.76      0.76      3200\n",
      "weighted avg       0.77      0.76      0.76      3200\n",
      "\n",
      "Test:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.68      0.74       407\n",
      "           1       0.71      0.82      0.77       393\n",
      "\n",
      "    accuracy                           0.75       800\n",
      "   macro avg       0.76      0.75      0.75       800\n",
      "weighted avg       0.76      0.75      0.75       800\n",
      "\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "===================DT Performance=====================\n",
      "95% CI = [ 0.7227608177074024 0.7628641822925973 ]\n",
      "Train:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.89      0.93      1593\n",
      "           1       0.90      0.98      0.94      1607\n",
      "\n",
      "    accuracy                           0.93      3200\n",
      "   macro avg       0.94      0.93      0.93      3200\n",
      "weighted avg       0.94      0.93      0.93      3200\n",
      "\n",
      "Test:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.72      0.75       407\n",
      "           1       0.73      0.78      0.76       393\n",
      "\n",
      "    accuracy                           0.75       800\n",
      "   macro avg       0.75      0.75      0.75       800\n",
      "weighted avg       0.75      0.75      0.75       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_models(equal, 'doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6301686d-4796-41e2-adda-8a063774b16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q7/lxfgdwq942qdsd3_mlwlm7p00000gn/T/ipykernel_62893/3287934322.py:6: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "===================MNB Performance=====================\n",
      "95% CI = [ 0.7239073216747745 0.7460926783252257 ]\n",
      "Train:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.68      0.73      1593\n",
      "           1       0.72      0.81      0.76      1607\n",
      "\n",
      "    accuracy                           0.74      3200\n",
      "   macro avg       0.75      0.74      0.74      3200\n",
      "weighted avg       0.75      0.74      0.74      3200\n",
      "\n",
      "Test:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.69      0.72       407\n",
      "           1       0.71      0.78      0.74       393\n",
      "\n",
      "    accuracy                           0.73       800\n",
      "   macro avg       0.73      0.73      0.73       800\n",
      "weighted avg       0.74      0.73      0.73       800\n",
      "\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "===================DT Performance=====================\n",
      "95% CI = [ 0.6663834146682985 0.7161165853317017 ]\n",
      "Train:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.76      0.84      1593\n",
      "           1       0.80      0.94      0.86      1607\n",
      "\n",
      "    accuracy                           0.85      3200\n",
      "   macro avg       0.86      0.85      0.85      3200\n",
      "weighted avg       0.86      0.85      0.85      3200\n",
      "\n",
      "Test:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.59      0.65       407\n",
      "           1       0.64      0.77      0.70       393\n",
      "\n",
      "    accuracy                           0.68       800\n",
      "   macro avg       0.68      0.68      0.67       800\n",
      "weighted avg       0.68      0.68      0.67       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_models(equal, 'tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20247209-ac3e-4f13-9ca2-154a830e48ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
