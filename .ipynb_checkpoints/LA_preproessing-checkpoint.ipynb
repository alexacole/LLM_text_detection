{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5db0976e-422a-4861-be36-0d77394a2346",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\color\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\color\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\color\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "import sklearn\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import scipy.stats as stats\n",
    "#import spacy\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75b5a047-4688-47da-af77-705569e87c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['source', 'id', 'text'],\n",
       "        num_rows: 1392522\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset('artem9k/ai-text-detection-pile')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a85aadf-da6e-49da-a4a7-9987e49b454c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>human</td>\n",
       "      <td>0</td>\n",
       "      <td>12 Years a Slave: An Analysis of the Film Essa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>human</td>\n",
       "      <td>1</td>\n",
       "      <td>20+ Social Media Post Ideas to Radically Simpl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>human</td>\n",
       "      <td>2</td>\n",
       "      <td>2022 Russian Invasion of Ukraine in Global Med...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>human</td>\n",
       "      <td>3</td>\n",
       "      <td>533 U.S. 27 (2001) Kyllo v. United States: The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>human</td>\n",
       "      <td>4</td>\n",
       "      <td>A Charles Schwab Corporation Case Essay\\n\\nCha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source  id                                               text\n",
       "0  human   0  12 Years a Slave: An Analysis of the Film Essa...\n",
       "1  human   1  20+ Social Media Post Ideas to Radically Simpl...\n",
       "2  human   2  2022 Russian Invasion of Ukraine in Global Med...\n",
       "3  human   3  533 U.S. 27 (2001) Kyllo v. United States: The...\n",
       "4  human   4  A Charles Schwab Corporation Case Essay\\n\\nCha..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(dataset['train'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f19051cc-1b32-4871-b433-537c40822f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source\n",
       "human    1028146\n",
       "ai        364376\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.source.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc858b14-9dbe-470f-928f-90881e776f13",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32b518de-439b-4bcc-93e2-534898f17a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for preprocessing\n",
    "def remove_urls(text):\n",
    "    return re.sub(r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\", \" \", text) # regex taken from https://www.geeksforgeeks.org/python-check-url-string/\n",
    "\n",
    "def remove_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def remove_extra_whitespace(text):\n",
    "    text = text.strip()\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def lemmatizer(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    l = nltk.stem.WordNetLemmatizer()\n",
    "    tokens = [l.lemmatize(token) for token in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def tokenize_pre_process(text): # for preprocessing using this link: https://spotintelligence.com/2022/12/21/nltk-preprocessing-pipeline/\n",
    "    # tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # remove stop words\n",
    "    stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "\n",
    "    # remove top 10% most frequent words \n",
    "    fdist = nltk.FreqDist(tokens)\n",
    "    tokens = [token for token in tokens if fdist[token] < fdist.N() * 0.1]\n",
    "\n",
    "    # stemming\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    # eliminate punctuation\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8857fa40-58c8-4c7b-a125-075104c50009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # encoding to ascii\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    \n",
    "    # convert text to lower case\n",
    "    text = text.lower()\n",
    "\n",
    "    # remove html tags \n",
    "    text = remove_html(text)\n",
    "\n",
    "    # remove urls \n",
    "    text = remove_urls(text)\n",
    "\n",
    "    # remove extra whitespace\n",
    "    text = remove_extra_whitespace(text)\n",
    "\n",
    "    # remove stop words\n",
    "    text = remove_stop_words(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1afd640-75a0-4bae-95ce-bc0a5a4b8360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text2(text):\n",
    "    # encoding to ascii\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    \n",
    "    # convert text to lower case\n",
    "    text = text.lower()\n",
    "\n",
    "    # remove html tags \n",
    "    text = remove_html(text)\n",
    "\n",
    "    # remove urls \n",
    "    text = remove_urls(text)\n",
    "\n",
    "    # remove extra whitespace\n",
    "    text = remove_extra_whitespace(text)\n",
    "\n",
    "    # remove stop words\n",
    "    text = remove_stop_words(text)\n",
    "\n",
    "    # lemmatize words\n",
    "    text = lemmatizer(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343436b3-c476-4b04-babb-29588e7cb236",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b5910b-2c22-4289-9b51-334e25c0fb26",
   "metadata": {},
   "source": [
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "294c2ed8-1f0f-4bca-ab6e-5392c1e64b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12b21e42-cb07-4f10-ab2a-c677818219f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(max_df=0.9,min_df=0.1)\n",
    "X = vec.fit_transform(df.text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c571910e-1b05-456e-921f-4b523ecb2fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 615)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69e51d74-f9c4-47b5-ac77-9995c56eafea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['10', '11', '12', '15', '19', '2017', '2018', '2019', '2020',\n",
       "       '2021', '2022', 'ability', 'able', 'about', 'above', 'access',\n",
       "       'according', 'achieve', 'across', 'act', 'action', 'actions',\n",
       "       'activities', 'activity', 'addition', 'additional', 'additionally',\n",
       "       'address', 'affect', 'affected', 'affects', 'after', 'against',\n",
       "       'age', 'al', 'all', 'allow', 'allowed', 'allows', 'almost',\n",
       "       'already', 'also', 'although', 'always', 'america', 'american',\n",
       "       'americans', 'among', 'an', 'analysis', 'another', 'any',\n",
       "       'approach', 'approaches', 'appropriate', 'are', 'area', 'areas',\n",
       "       'around', 'article', 'aspect', 'aspects', 'associated', 'at',\n",
       "       'attention', 'author', 'available', 'avoid', 'back', 'based',\n",
       "       'basis', 'be', 'became', 'because', 'become', 'becomes', 'been',\n",
       "       'before', 'behavior', 'being', 'believe', 'benefits', 'best',\n",
       "       'better', 'between', 'black', 'body', 'both', 'business', 'but',\n",
       "       'by', 'can', 'cannot', 'care', 'case', 'cases', 'cause', 'caused',\n",
       "       'causes', 'central', 'century', 'certain', 'challenges', 'change',\n",
       "       'changes', 'characteristics', 'children', 'cited', 'citizens',\n",
       "       'clear', 'close', 'come', 'common', 'communication', 'community',\n",
       "       'companies', 'company', 'compared', 'complex', 'concept',\n",
       "       'concerns', 'conclusion', 'condition', 'conditions',\n",
       "       'consequences', 'consider', 'considered', 'considering',\n",
       "       'contents', 'context', 'contribute', 'control', 'cost', 'costs',\n",
       "       'could', 'countries', 'country', 'covid', 'create', 'created',\n",
       "       'creating', 'critical', 'crucial', 'cultural', 'culture',\n",
       "       'current', 'data', 'day', 'decision', 'decisions', 'despite',\n",
       "       'determine', 'develop', 'developed', 'developing', 'development',\n",
       "       'did', 'differences', 'different', 'difficult', 'direct',\n",
       "       'directly', 'discussion', 'disease', 'do', 'does', 'due', 'during',\n",
       "       'each', 'early', 'economic', 'education', 'effect', 'effective',\n",
       "       'effectively', 'effects', 'elements', 'employees', 'end', 'enough',\n",
       "       'ensure', 'environment', 'especially', 'essay', 'essential',\n",
       "       'established', 'et', 'even', 'events', 'every', 'everyone',\n",
       "       'evidence', 'example', 'existing', 'experience', 'face', 'fact',\n",
       "       'factor', 'factors', 'family', 'features', 'feel', 'few', 'field',\n",
       "       'finally', 'financial', 'find', 'first', 'five', 'focus', 'follow',\n",
       "       'following', 'food', 'form', 'forms', 'found', 'framework', 'free',\n",
       "       'from', 'full', 'further', 'furthermore', 'future', 'general',\n",
       "       'get', 'give', 'given', 'global', 'go', 'goal', 'goals', 'good',\n",
       "       'government', 'great', 'group', 'groups', 'growth', 'had', 'hand',\n",
       "       'has', 'have', 'having', 'he', 'health', 'healthcare', 'help',\n",
       "       'helps', 'hence', 'her', 'high', 'higher', 'highly', 'him', 'his',\n",
       "       'history', 'home', 'how', 'however', 'human', 'idea', 'ideas',\n",
       "       'identify', 'if', 'impact', 'importance', 'important', 'improve',\n",
       "       'include', 'includes', 'including', 'increase', 'increased',\n",
       "       'increasing', 'individual', 'individuals', 'industry', 'influence',\n",
       "       'information', 'instance', 'instead', 'interest', 'international',\n",
       "       'into', 'introduction', 'involved', 'issue', 'issues', 'its',\n",
       "       'itself', 'journal', 'just', 'key', 'know', 'knowledge', 'known',\n",
       "       'lack', 'large', 'last', 'later', 'law', 'lead', 'leading',\n",
       "       'leads', 'learning', 'led', 'legal', 'less', 'level', 'levels',\n",
       "       'life', 'like', 'likely', 'limited', 'live', 'lives', 'living',\n",
       "       'local', 'long', 'low', 'lower', 'made', 'main', 'maintain',\n",
       "       'major', 'make', 'makes', 'making', 'man', 'management', 'many',\n",
       "       'market', 'matter', 'may', 'me', 'meaning', 'means', 'measures',\n",
       "       'media', 'medical', 'members', 'mental', 'mentioned', 'method',\n",
       "       'methods', 'might', 'model', 'modern', 'money', 'more', 'moreover',\n",
       "       'most', 'much', 'multiple', 'must', 'my', 'national', 'natural',\n",
       "       'nature', 'necessary', 'need', 'needed', 'needs', 'negative',\n",
       "       'never', 'new', 'next', 'no', 'non', 'not', 'now', 'number',\n",
       "       'numerous', 'often', 'one', 'ones', 'only', 'operations',\n",
       "       'opinion', 'opportunities', 'opportunity', 'or', 'order',\n",
       "       'organization', 'organizations', 'other', 'others', 'our', 'out',\n",
       "       'outcomes', 'over', 'overall', 'own', 'pandemic', 'paper', 'part',\n",
       "       'particular', 'particularly', 'past', 'patient', 'patients',\n",
       "       'people', 'performance', 'period', 'person', 'personal',\n",
       "       'perspective', 'physical', 'place', 'plan', 'play', 'point',\n",
       "       'policy', 'political', 'population', 'position', 'positive',\n",
       "       'possible', 'potential', 'power', 'pp', 'practice', 'practices',\n",
       "       'present', 'presented', 'prevent', 'primary', 'principles',\n",
       "       'problem', 'problems', 'process', 'processes', 'product',\n",
       "       'production', 'products', 'professional', 'promote', 'proper',\n",
       "       'provide', 'provided', 'provides', 'providing', 'public',\n",
       "       'purpose', 'quality', 'question', 'range', 'rate', 'rather',\n",
       "       'real', 'reason', 'reasons', 'receive', 'reduce', 'reference',\n",
       "       'references', 'regarding', 'related', 'relationship',\n",
       "       'relationships', 'relevant', 'report', 'require', 'required',\n",
       "       'requires', 'research', 'resources', 'responsibility',\n",
       "       'responsible', 'result', 'results', 'review', 'right', 'rights',\n",
       "       'risk', 'risks', 'role', 'safety', 'same', 'science', 'second',\n",
       "       'security', 'see', 'seen', 'self', 'service', 'services', 'set',\n",
       "       'several', 'severe', 'share', 'she', 'should', 'show', 'shows',\n",
       "       'significant', 'significantly', 'similar', 'since', 'situation',\n",
       "       'skills', 'small', 'so', 'social', 'society', 'some', 'source',\n",
       "       'sources', 'specific', 'spread', 'state', 'states', 'status',\n",
       "       'still', 'story', 'strategies', 'strategy', 'strong', 'structure',\n",
       "       'studies', 'study', 'subject', 'success', 'successful', 'such',\n",
       "       'support', 'system', 'systems', 'table', 'take', 'taken', 'taking',\n",
       "       'technology', 'term', 'terms', 'than', 'their', 'them',\n",
       "       'themselves', 'then', 'theory', 'there', 'therefore', 'these',\n",
       "       'they', 'third', 'this', 'those', 'though', 'three', 'through',\n",
       "       'throughout', 'thus', 'time', 'times', 'today', 'together',\n",
       "       'topic', 'towards', 'treatment', 'turn', 'two', 'type', 'types',\n",
       "       'under', 'understand', 'understanding', 'unique', 'united',\n",
       "       'university', 'up', 'us', 'use', 'used', 'uses', 'using',\n",
       "       'usually', 'value', 'values', 'various', 'very', 'view', 'vital',\n",
       "       'want', 'was', 'way', 'ways', 'we', 'web', 'well', 'were', 'what',\n",
       "       'when', 'where', 'whether', 'which', 'while', 'white', 'who',\n",
       "       'whole', 'why', 'will', 'within', 'without', 'women', 'words',\n",
       "       'work', 'workers', 'working', 'works', 'world', 'would', 'year',\n",
       "       'years', 'you', 'young'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f22d6b9-f08c-4ac8-a8fa-6598f213a5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec2 = CountVectorizer(preprocessor=preprocess_text,max_df=0.9,min_df=0.1)\n",
    "X2 = vec2.fit_transform(df.text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be6ff840-a8e5-4cfe-86ec-2eefd27edc34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['10', '11', '12', '15', '19', '2017', '2018', '2019', '2020',\n",
       "       '2021', '2022', 'ability', 'able', 'access', 'according',\n",
       "       'achieve', 'across', 'act', 'action', 'actions', 'activities',\n",
       "       'activity', 'addition', 'additional', 'additionally', 'address',\n",
       "       'affect', 'affected', 'affects', 'age', 'al', 'allow', 'allowed',\n",
       "       'allows', 'almost', 'already', 'also', 'although', 'always',\n",
       "       'america', 'american', 'americans', 'among', 'analysis', 'another',\n",
       "       'approach', 'approaches', 'appropriate', 'area', 'areas', 'around',\n",
       "       'article', 'aspect', 'aspects', 'associated', 'attention',\n",
       "       'author', 'authors', 'available', 'avoid', 'back', 'based',\n",
       "       'basis', 'became', 'become', 'becomes', 'behavior', 'believe',\n",
       "       'benefits', 'best', 'better', 'body', 'business', 'care', 'case',\n",
       "       'cases', 'cause', 'caused', 'causes', 'central', 'century',\n",
       "       'certain', 'challenges', 'change', 'changes', 'characteristics',\n",
       "       'children', 'cited', 'citizens', 'clear', 'close', 'come',\n",
       "       'common', 'communication', 'community', 'companies', 'company',\n",
       "       'companys', 'compared', 'complex', 'concept', 'concerns',\n",
       "       'conclusion', 'condition', 'conditions', 'consequences',\n",
       "       'consider', 'considered', 'considering', 'contents', 'context',\n",
       "       'contribute', 'control', 'cost', 'costs', 'could', 'countries',\n",
       "       'country', 'covid', 'create', 'created', 'creating', 'critical',\n",
       "       'crucial', 'cultural', 'culture', 'current', 'data', 'day',\n",
       "       'decision', 'decisions', 'despite', 'determine', 'develop',\n",
       "       'developed', 'developing', 'development', 'differences',\n",
       "       'different', 'difficult', 'direct', 'directly', 'discussion',\n",
       "       'disease', 'due', 'early', 'economic', 'education', 'effect',\n",
       "       'effective', 'effectively', 'effects', 'elements', 'employees',\n",
       "       'end', 'enough', 'ensure', 'environment', 'especially', 'essay',\n",
       "       'essential', 'established', 'et', 'even', 'events', 'every',\n",
       "       'evidence', 'example', 'existing', 'experience', 'face', 'fact',\n",
       "       'factor', 'factors', 'family', 'features', 'feel', 'field',\n",
       "       'finally', 'financial', 'find', 'first', 'five', 'focus', 'follow',\n",
       "       'following', 'food', 'form', 'forms', 'found', 'framework', 'free',\n",
       "       'full', 'furthermore', 'future', 'general', 'get', 'give', 'given',\n",
       "       'global', 'go', 'goal', 'goals', 'good', 'government', 'great',\n",
       "       'group', 'groups', 'growth', 'hand', 'health', 'healthcare',\n",
       "       'help', 'helps', 'hence', 'high', 'higher', 'highly', 'history',\n",
       "       'home', 'however', 'human', 'idea', 'ideas', 'identify', 'impact',\n",
       "       'importance', 'important', 'improve', 'include', 'includes',\n",
       "       'including', 'increase', 'increased', 'increasing', 'individual',\n",
       "       'individuals', 'industry', 'influence', 'information', 'instance',\n",
       "       'instead', 'interest', 'international', 'introduction', 'involved',\n",
       "       'issue', 'issues', 'journal', 'key', 'know', 'knowledge', 'known',\n",
       "       'lack', 'large', 'last', 'later', 'law', 'lead', 'leading',\n",
       "       'leads', 'learning', 'led', 'legal', 'less', 'level', 'levels',\n",
       "       'life', 'like', 'likely', 'limited', 'live', 'lives', 'living',\n",
       "       'local', 'long', 'low', 'lower', 'made', 'main', 'maintain',\n",
       "       'major', 'make', 'makes', 'making', 'management', 'many', 'market',\n",
       "       'matter', 'may', 'meaning', 'means', 'measures', 'media',\n",
       "       'medical', 'members', 'mental', 'mentioned', 'method', 'methods',\n",
       "       'might', 'model', 'modern', 'money', 'moreover', 'much',\n",
       "       'multiple', 'must', 'national', 'natural', 'nature', 'necessary',\n",
       "       'need', 'needed', 'needs', 'negative', 'never', 'new', 'next',\n",
       "       'non', 'number', 'numerous', 'often', 'one', 'ones', 'operations',\n",
       "       'opinion', 'opportunities', 'opportunity', 'order', 'organization',\n",
       "       'organizations', 'others', 'outcomes', 'overall', 'pandemic',\n",
       "       'paper', 'part', 'particular', 'particularly', 'past', 'patient',\n",
       "       'patients', 'people', 'peoples', 'performance', 'period', 'person',\n",
       "       'personal', 'persons', 'perspective', 'physical', 'place', 'plan',\n",
       "       'play', 'point', 'policy', 'political', 'population', 'position',\n",
       "       'positive', 'possible', 'potential', 'power', 'pp', 'practice',\n",
       "       'practices', 'present', 'presented', 'prevent', 'primary',\n",
       "       'principles', 'problem', 'problems', 'process', 'processes',\n",
       "       'product', 'production', 'products', 'professional', 'programs',\n",
       "       'promote', 'proper', 'provide', 'provided', 'provides',\n",
       "       'providing', 'public', 'purpose', 'quality', 'question', 'range',\n",
       "       'rate', 'rather', 'real', 'reason', 'reasons', 'receive', 'reduce',\n",
       "       'reference', 'references', 'regarding', 'related', 'relationship',\n",
       "       'relationships', 'relevant', 'report', 'require', 'required',\n",
       "       'requires', 'research', 'resources', 'responsibility',\n",
       "       'responsible', 'result', 'results', 'review', 'right', 'rights',\n",
       "       'risk', 'risks', 'role', 'safety', 'science', 'second', 'security',\n",
       "       'see', 'seen', 'self', 'service', 'services', 'set', 'several',\n",
       "       'severe', 'share', 'show', 'shows', 'significant', 'significantly',\n",
       "       'similar', 'since', 'situation', 'skills', 'small', 'social',\n",
       "       'society', 'source', 'sources', 'specific', 'spread', 'state',\n",
       "       'states', 'status', 'still', 'strategies', 'strategy', 'strong',\n",
       "       'structure', 'studies', 'study', 'subject', 'success',\n",
       "       'successful', 'support', 'system', 'systems', 'table', 'take',\n",
       "       'taken', 'taking', 'technology', 'term', 'terms', 'theory',\n",
       "       'therefore', 'third', 'though', 'three', 'throughout', 'thus',\n",
       "       'time', 'times', 'together', 'topic', 'towards', 'treatment',\n",
       "       'turn', 'two', 'type', 'types', 'understand', 'understanding',\n",
       "       'unique', 'united', 'university', 'us', 'use', 'used', 'uses',\n",
       "       'using', 'usually', 'value', 'values', 'various', 'view', 'vital',\n",
       "       'want', 'way', 'ways', 'web', 'well', 'whether', 'white', 'whole',\n",
       "       'within', 'without', 'women', 'words', 'work', 'workers',\n",
       "       'working', 'works', 'world', 'would', 'year', 'years', 'young'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec2.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "babe2fe5-7e00-49a3-928d-4dae81182844",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec3 = CountVectorizer(preprocessor=preprocess_text2,max_df=0.9,min_df=0.1)\n",
    "X3 = vec3.fit_transform(df.text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4165a5c5-cea7-42fc-9d2a-fd5f1e413640",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['10', '11', '12', '15', '19', '2017', '2018', '2019', '2020',\n",
       "       '2021', '2022', 'ability', 'able', 'access', 'according',\n",
       "       'account', 'achieve', 'across', 'act', 'action', 'activity',\n",
       "       'addition', 'additional', 'additionally', 'address', 'advantage',\n",
       "       'affect', 'affected', 'age', 'aim', 'al', 'allow', 'allowed',\n",
       "       'allows', 'almost', 'already', 'also', 'although', 'always',\n",
       "       'america', 'american', 'among', 'amount', 'analysis', 'another',\n",
       "       'application', 'approach', 'appropriate', 'area', 'around',\n",
       "       'article', 'aspect', 'assessment', 'associated', 'attention',\n",
       "       'attitude', 'author', 'authority', 'available', 'avoid', 'back',\n",
       "       'background', 'based', 'basis', 'became', 'become', 'becomes',\n",
       "       'behavior', 'being', 'belief', 'believe', 'benefit', 'best',\n",
       "       'better', 'black', 'body', 'book', 'business', 'care', 'case',\n",
       "       'cause', 'caused', 'center', 'central', 'century', 'certain',\n",
       "       'challenge', 'chance', 'change', 'character', 'characteristic',\n",
       "       'child', 'choice', 'cited', 'citizen', 'claim', 'clear', 'close',\n",
       "       'come', 'common', 'communication', 'community', 'company',\n",
       "       'compared', 'complex', 'component', 'concept', 'concern',\n",
       "       'conclusion', 'condition', 'conduct', 'conflict', 'connection',\n",
       "       'consequence', 'consider', 'consideration', 'considered',\n",
       "       'considering', 'content', 'context', 'contribute', 'control',\n",
       "       'cost', 'could', 'country', 'covid', 'create', 'created',\n",
       "       'creating', 'critical', 'crucial', 'cultural', 'culture',\n",
       "       'current', 'customer', 'data', 'day', 'death', 'decision',\n",
       "       'demand', 'desire', 'despite', 'detail', 'determine', 'develop',\n",
       "       'developed', 'developing', 'development', 'difference',\n",
       "       'different', 'difficult', 'direct', 'directly', 'discussion',\n",
       "       'disease', 'due', 'early', 'economic', 'ed', 'education', 'effect',\n",
       "       'effective', 'effectively', 'effort', 'element', 'employee', 'end',\n",
       "       'enough', 'ensure', 'environment', 'especially', 'essay',\n",
       "       'essential', 'established', 'et', 'even', 'event', 'every',\n",
       "       'evidence', 'example', 'existing', 'experience', 'face', 'fact',\n",
       "       'factor', 'family', 'feature', 'feel', 'feeling', 'field',\n",
       "       'finally', 'financial', 'find', 'finding', 'first', 'five',\n",
       "       'focus', 'follow', 'following', 'food', 'force', 'form', 'found',\n",
       "       'framework', 'free', 'full', 'function', 'furthermore', 'future',\n",
       "       'general', 'get', 'give', 'given', 'global', 'go', 'goal', 'good',\n",
       "       'government', 'great', 'group', 'growth', 'hand', 'health',\n",
       "       'healthcare', 'help', 'hence', 'high', 'higher', 'highly',\n",
       "       'history', 'home', 'however', 'human', 'idea', 'identify', 'image',\n",
       "       'impact', 'importance', 'important', 'improve', 'improvement',\n",
       "       'include', 'includes', 'including', 'income', 'increase',\n",
       "       'increased', 'increasing', 'individual', 'industry', 'influence',\n",
       "       'information', 'instance', 'instead', 'interaction', 'interest',\n",
       "       'international', 'introduction', 'involved', 'issue', 'job',\n",
       "       'journal', 'keep', 'key', 'know', 'knowledge', 'known', 'lack',\n",
       "       'large', 'last', 'later', 'law', 'lead', 'leader', 'leading',\n",
       "       'learning', 'led', 'legal', 'less', 'level', 'life', 'like',\n",
       "       'likely', 'limited', 'live', 'living', 'local', 'long', 'look',\n",
       "       'loss', 'low', 'lower', 'made', 'main', 'maintain', 'major',\n",
       "       'make', 'making', 'man', 'management', 'many', 'market',\n",
       "       'material', 'matter', 'may', 'mean', 'meaning', 'measure',\n",
       "       'medical', 'medicine', 'medium', 'meet', 'member', 'mental',\n",
       "       'mentioned', 'method', 'might', 'million', 'model', 'modern',\n",
       "       'money', 'moreover', 'movement', 'much', 'multiple', 'must',\n",
       "       'national', 'natural', 'nature', 'necessary', 'need', 'needed',\n",
       "       'negative', 'never', 'new', 'next', 'non', 'note', 'number',\n",
       "       'numerous', 'objective', 'offer', 'often', 'one', 'open',\n",
       "       'operation', 'opinion', 'opportunity', 'order', 'organization',\n",
       "       'others', 'outcome', 'overall', 'pandemic', 'paper', 'part',\n",
       "       'particular', 'particularly', 'past', 'patient', 'pay', 'people',\n",
       "       'perception', 'performance', 'period', 'person', 'personal',\n",
       "       'perspective', 'physical', 'place', 'plan', 'play', 'point',\n",
       "       'policy', 'political', 'population', 'position', 'positive',\n",
       "       'possibility', 'possible', 'potential', 'power', 'pp', 'practice',\n",
       "       'present', 'presented', 'press', 'prevent', 'primary', 'principle',\n",
       "       'problem', 'process', 'product', 'production', 'professional',\n",
       "       'program', 'project', 'promote', 'proper', 'provide', 'provided',\n",
       "       'provides', 'providing', 'public', 'purpose', 'put', 'quality',\n",
       "       'question', 'range', 'rate', 'rather', 'real', 'reason', 'receive',\n",
       "       'reduce', 'reference', 'regarding', 'related', 'relation',\n",
       "       'relationship', 'relevant', 'report', 'require', 'required',\n",
       "       'requires', 'research', 'researcher', 'resource', 'response',\n",
       "       'responsibility', 'responsible', 'result', 'review', 'right',\n",
       "       'risk', 'role', 'rule', 'safety', 'say', 'science', 'second',\n",
       "       'security', 'see', 'seen', 'self', 'service', 'set', 'setting',\n",
       "       'several', 'severe', 'share', 'show', 'side', 'significant',\n",
       "       'significantly', 'similar', 'since', 'situation', 'skill', 'small',\n",
       "       'social', 'society', 'solution', 'source', 'specific', 'spread',\n",
       "       'stage', 'standard', 'start', 'state', 'statement', 'status',\n",
       "       'step', 'still', 'story', 'strategy', 'strong', 'structure',\n",
       "       'study', 'subject', 'success', 'successful', 'support', 'system',\n",
       "       'table', 'take', 'taken', 'taking', 'task', 'technology', 'term',\n",
       "       'theory', 'therefore', 'thing', 'think', 'third', 'though',\n",
       "       'thought', 'threat', 'three', 'throughout', 'thus', 'time',\n",
       "       'today', 'together', 'tool', 'topic', 'towards', 'treatment',\n",
       "       'trend', 'turn', 'two', 'type', 'understand', 'understanding',\n",
       "       'unique', 'united', 'university', 'us', 'use', 'used', 'using',\n",
       "       'usually', 'value', 'various', 'view', 'vital', 'want', 'way',\n",
       "       'web', 'well', 'whether', 'white', 'whole', 'within', 'without',\n",
       "       'woman', 'word', 'work', 'worker', 'working', 'world', 'would',\n",
       "       'writing', 'year', 'young'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec3.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "368a78d5-0e20-4500-8d23-7bac8accebf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['al 2020', 'content introduction', 'covid 19', 'essay table',\n",
       "       'essay table content', 'et al', 'et al 2020', 'research paper',\n",
       "       'table content', 'table content introduction', 'united state',\n",
       "       'work cited'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec4 = CountVectorizer(preprocessor=preprocess_text2,max_df=0.9,min_df=0.1, ngram_range=(2,3))\n",
    "X4 = vec4.fit_transform(df.text[:1000])\n",
    "vec4.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cd1285-0e3e-4274-8163-dd0425dfb588",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cb2a20f-7761-417c-a22b-0c53a82c8358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(data_train, data_test):\n",
    "    tfidf = TfidfVectorizer(preprocessor=preprocess_text2,max_df=0.9,min_df=0.1)\n",
    "    train = tfidf.fit_transform(data_train)\n",
    "    test = tfidf.transform(data_test)\n",
    "    return train, test "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ad9fab-ff72-4a2f-a7f8-e44bf51f062e",
   "metadata": {},
   "source": [
    "### Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b505b72-5113-4ebc-945d-17283978efdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76b00d20-428b-47a5-8b29-82a20c4d17c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc2vec(data):\n",
    "    '''\n",
    "    https://www.geeksforgeeks.org/doc2vec-in-nlp/\n",
    "    '''\n",
    "    \n",
    "    tagged_data = [TaggedDocument(words=word_tokenize(doc.lower()),\n",
    "                              tags=[str(i)]) for i,doc in enumerate(data)]\n",
    "    # train the Doc2vec model\n",
    "    model = Doc2Vec(vector_size=20,\n",
    "                    min_count=2, epochs=50)\n",
    "    model.build_vocab(tagged_data)\n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.epochs)\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_vectors(doc2vec_model, data):\n",
    "    # get the document vectors\n",
    "    return [doc2vec_model.infer_vector(word_tokenize(doc.lower())) for doc in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a152068c-4e66-47ef-b31b-b9b44255a3ea",
   "metadata": {},
   "source": [
    "### Experimentation with doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "a820c507-856d-4d7b-a8c5-1ead079eb7d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.doc2vec.Doc2Vec at 0x22782e567f0>"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m=doc2vec(data.text)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "244b91bd-6480-4be6-8ff5-11ee26ee577a",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = get_vectors(m, data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "da9a0ca5-d71e-4e6b-b2d7-09e65d36127f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, t_train, t_test = train_test_split(np.array(data.text), t, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "18aea95e-2d46-4d56-8587-5f74c6abb376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1113,)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "a3134d32-4871-4c3c-ae9c-7505d42bab9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\color\\AppData\\Local\\Temp\\ipykernel_23348\\3287934322.py:6: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    }
   ],
   "source": [
    "tex = []\n",
    "for i in range(len(X_train)):\n",
    "    tex += [preprocess_text2(X_train[i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "1ef4441d-7aed-4fc9-beb2-7c560853aa8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1113, 20)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = doc2vec(tex)\n",
    "X_train = np.array(get_vectors(m, tex))\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "36fe2d55-6b18-46db-93dc-147e543740a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(279, 20)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = np.array(get_vectors(m, X_test))\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3e90b9-33be-48ed-8ce8-2ceec66309fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1a7562a-3e07-4f15-ab70-5228896fe154",
   "metadata": {},
   "source": [
    "## Sampling from the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810c3b8a-4af6-48ac-ab6d-8765ecff56e9",
   "metadata": {},
   "source": [
    "### Preserving the original proportions of humans and ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8dc50736-666f-46f5-b7f1-8e4a03410827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent ai: 35.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\color\\AppData\\Local\\Temp\\ipykernel_21624\\875810769.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  data = df.groupby('source', group_keys=False).apply(lambda x: x.sample(frac=0.001, random_state=0))\n"
     ]
    }
   ],
   "source": [
    "# Reporting the proportion of samples that are ai generated\n",
    "print('Percent ai:', round(df[df['source'] == 'ai'].shape[0]/df[df['source'] == 'human'].shape[0]*100, 3))\n",
    "\n",
    "# Taking a stratified sample of 0.1% of the data\n",
    "# maintaining same proportions of human and ai samples\n",
    "data = df.groupby('source', group_keys=False).apply(lambda x: x.sample(frac=0.001, random_state=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60b2f4c-a92b-4247-bc9f-3c9120185e6e",
   "metadata": {},
   "source": [
    "### Randomly sampling 2000 samples each of human and ai sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "526cb205-3d63-41d8-85fd-b056e24105c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>human</td>\n",
       "      <td>198049</td>\n",
       "      <td>Overview\\n\\nBatman and Psychology: A Dark and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>human</td>\n",
       "      <td>12919</td>\n",
       "      <td>The Use of Psychedelic Drugs in Treating Depre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>human</td>\n",
       "      <td>979845</td>\n",
       "      <td>member the day like it was yesterday. \\n My mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>human</td>\n",
       "      <td>73499</td>\n",
       "      <td>Legislative Branch Power, Its Limits and Expan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>human</td>\n",
       "      <td>44380</td>\n",
       "      <td>Growth and Fall of Vader Corporation Report\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>ai</td>\n",
       "      <td>1052501</td>\n",
       "      <td>\"Learn Python the Hard Way\" by Zed Shaw\\n\\t\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>ai</td>\n",
       "      <td>1277472</td>\n",
       "      <td>Cape Town - The South African Humanist Associa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>ai</td>\n",
       "      <td>1086015</td>\n",
       "      <td>We are only days away from the 2016 NFL Draft,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>ai</td>\n",
       "      <td>1338694</td>\n",
       "      <td>The number of people infected with Zika virus ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>ai</td>\n",
       "      <td>1189675</td>\n",
       "      <td>Dorothy Lee Hough - May 24, 2017\\n\\nDorothy Le...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     source       id                                               text\n",
       "0     human   198049  Overview\\n\\nBatman and Psychology: A Dark and ...\n",
       "1     human    12919  The Use of Psychedelic Drugs in Treating Depre...\n",
       "2     human   979845  member the day like it was yesterday. \\n My mo...\n",
       "3     human    73499  Legislative Branch Power, Its Limits and Expan...\n",
       "4     human    44380  Growth and Fall of Vader Corporation Report\\n\\...\n",
       "...     ...      ...                                                ...\n",
       "3995     ai  1052501   \"Learn Python the Hard Way\" by Zed Shaw\\n\\t\\t...\n",
       "3996     ai  1277472  Cape Town - The South African Humanist Associa...\n",
       "3997     ai  1086015  We are only days away from the 2016 NFL Draft,...\n",
       "3998     ai  1338694  The number of people infected with Zika virus ...\n",
       "3999     ai  1189675  Dorothy Lee Hough - May 24, 2017\\n\\nDorothy Le...\n",
       "\n",
       "[4000 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly sampling 2000 human and 2000 ai samples to create dataset with equal proportions\n",
    "human = df[df.source == \"human\"]\n",
    "human = human.sample(2000, random_state=0)\n",
    "ai = df[df.source == \"ai\"].sample(2000, random_state=0)\n",
    "equal = pd.concat([human,ai], ignore_index=True)\n",
    "equal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba83c9c4-2c3d-4634-85dc-74cb65e4c657",
   "metadata": {},
   "source": [
    "# Run Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b0ab80d-c08f-492f-bf2a-751acb2454ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CI(metric, confidence):\n",
    "    a,b = stats.t.interval(confidence, \n",
    "                         len(metric)-1, \n",
    "                         loc=metric.mean(), \n",
    "                         scale=metric.std(ddof=1)/np.sqrt(len(metric)))\n",
    "    return a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c739f1d-53ae-4c46-b02b-db9d32b0ab08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(X_train, X_test, t_train, t_test, model, model_name, confidence=0.95, scoring='accuracy'):\n",
    "\n",
    "    y_train = model.predict(X_train)\n",
    "    y_test = model.predict(X_test)\n",
    "    \n",
    "    scores = cross_val_score(model,\n",
    "                             X_train, \n",
    "                             t_train, \n",
    "                             scoring=scoring, \n",
    "                             cv=KFold(10, shuffle=True, random_state=0))\n",
    "    \n",
    "    a,b = CI(scores, confidence)\n",
    "    \n",
    "    print(f'==================={model_name} Performance=====================')\n",
    "    print('95% CI = [', a, b, ']')\n",
    "    print('Train: ', classification_report(t_train, y_train))\n",
    "    print('Test: ', classification_report(t_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16c36ae6-8b8a-4769-94c6-6483546da92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_models(data, features):\n",
    "    # Convert labels into numeric\n",
    "    t = data.source\n",
    "    d = {'human' : 0, 'ai' : 1}\n",
    "    t = t.map(d, na_action='ignore')\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, t_train, t_test = train_test_split(np.array(data.text), t, test_size=0.2, random_state=0)\n",
    "    \n",
    "    if features == 'tfidf':\n",
    "        X_train, X_test = tfidf(X_train,X_test)\n",
    "        X_train = X_train.toarray()\n",
    "        X_test = X_test.toarray()\n",
    "    elif features == 'doc2vec':\n",
    "        train_prep = []\n",
    "        test_prep = []\n",
    "        for i in range(len(X_train)):\n",
    "            train_prep += [preprocess_text2(X_train[i])]\n",
    "        for i in range(len(X_test)):\n",
    "            test_prep += [preprocess_text2(X_test[i])]\n",
    "        \n",
    "        m = doc2vec(train_prep)\n",
    "        X_train = np.array(get_vectors(m, train_prep))\n",
    "        X_test = np.array(get_vectors(m, test_prep))\n",
    "\n",
    "    # Naive Bayes\n",
    "    gnb = GaussianNB()\n",
    "    gnb.fit(X_train, t_train)\n",
    "    evaluation(X_train, X_test, t_train, t_test, gnb, 'Naive Bayes')\n",
    "\n",
    "    # Logistic Regression\n",
    "    lr_pipe = Pipeline([('log_reg', LogisticRegression(solver='saga'))])\n",
    "\n",
    "    param_grid1 = {'log_reg__C': [0.0001, 0.01, 0.1], \n",
    "                   'log_reg__penalty':[None,'l1','l2']}\n",
    "    \n",
    "    gs1 = GridSearchCV(lr_pipe, \n",
    "                       param_grid=param_grid1,\n",
    "                       cv=KFold(n_splits=5, shuffle=True, random_state=0), \n",
    "                       scoring='accuracy',\n",
    "                       verbose=1, \n",
    "                       n_jobs=-1, \n",
    "                       refit=True)\n",
    "    \n",
    "    gs1.fit(X_train, t_train)\n",
    "    lr = gs1.best_estimator_\n",
    "    evaluation(X_train, X_test, t_train, t_test, lr, 'Logistic Regression')\n",
    "\n",
    "    # KNN with PCA\n",
    "    knn_pipe = Pipeline([('pca', PCA()),\n",
    "                     ('knn', KNeighborsClassifier())])\n",
    "\n",
    "    param_grid2 = {'pca__n_components': [0.7, 0.8, 0.9], \n",
    "                   'knn__n_neighbors': [5,7,9]}\n",
    "    \n",
    "    gs2 = GridSearchCV(knn_pipe, \n",
    "                       param_grid=param_grid2,\n",
    "                       cv=KFold(n_splits=5, shuffle=True, random_state=0), \n",
    "                       scoring='accuracy',\n",
    "                       verbose=1, \n",
    "                       n_jobs=-1, \n",
    "                       refit=True)\n",
    "    \n",
    "    gs2.fit(X_train, t_train)\n",
    "    knn = gs2.best_estimator_\n",
    "    evaluation(X_train, X_test, t_train, t_test, knn, 'KNN with PCA')\n",
    "\n",
    "    # KNN without PCA\n",
    "    knn_pipe = Pipeline([('knn', KNeighborsClassifier())])\n",
    "\n",
    "    param_grid3 = {'knn__n_neighbors': [5,7,9]}\n",
    "    \n",
    "    gs3 = GridSearchCV(knn_pipe, \n",
    "                       param_grid=param_grid3,\n",
    "                       cv=KFold(n_splits=5, shuffle=True, random_state=0), \n",
    "                       scoring='accuracy',\n",
    "                       verbose=1, \n",
    "                       n_jobs=-1, \n",
    "                       refit=True)\n",
    "    \n",
    "    gs3.fit(X_train, t_train)\n",
    "    knn = gs3.best_estimator_\n",
    "    evaluation(X_train, X_test, t_train, t_test, knn, 'KNN without PCA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3dc7dc79-d3ca-4d5e-a0a8-c01ab0c5442c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\color\\AppData\\Local\\Temp\\ipykernel_21624\\3287934322.py:6: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================Naive Bayes Performance=====================\n",
      "95% CI = [ 0.7385418077168382 0.7889356697606396 ]\n",
      "Train:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.91      0.86       827\n",
      "           1       0.61      0.41      0.49       286\n",
      "\n",
      "    accuracy                           0.78      1113\n",
      "   macro avg       0.71      0.66      0.68      1113\n",
      "weighted avg       0.76      0.78      0.77      1113\n",
      "\n",
      "Test:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.97      0.85       201\n",
      "           1       0.70      0.21      0.32        78\n",
      "\n",
      "    accuracy                           0.75       279\n",
      "   macro avg       0.73      0.59      0.58       279\n",
      "weighted avg       0.74      0.75      0.70       279\n",
      "\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\color\\Anaconda3\\envs\\pr_project\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\color\\Anaconda3\\envs\\pr_project\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\color\\Anaconda3\\envs\\pr_project\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\color\\Anaconda3\\envs\\pr_project\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\color\\Anaconda3\\envs\\pr_project\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\color\\Anaconda3\\envs\\pr_project\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\color\\Anaconda3\\envs\\pr_project\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\color\\Anaconda3\\envs\\pr_project\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\color\\Anaconda3\\envs\\pr_project\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\color\\Anaconda3\\envs\\pr_project\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "C:\\Users\\color\\Anaconda3\\envs\\pr_project\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================Logistic Regression Performance=====================\n",
      "95% CI = [ 0.7556454175284176 0.7987947369117369 ]\n",
      "Train:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.93      0.87       827\n",
      "           1       0.65      0.37      0.47       286\n",
      "\n",
      "    accuracy                           0.79      1113\n",
      "   macro avg       0.73      0.65      0.67      1113\n",
      "weighted avg       0.77      0.79      0.77      1113\n",
      "\n",
      "Test:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.94      0.85       201\n",
      "           1       0.65      0.28      0.39        78\n",
      "\n",
      "    accuracy                           0.76       279\n",
      "   macro avg       0.71      0.61      0.62       279\n",
      "weighted avg       0.74      0.76      0.72       279\n",
      "\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "===================KNN with PCA Performance=====================\n",
      "95% CI = [ 0.7449804547782732 0.8166347318369134 ]\n",
      "Train:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.89      0.90       827\n",
      "           1       0.70      0.72      0.71       286\n",
      "\n",
      "    accuracy                           0.85      1113\n",
      "   macro avg       0.80      0.81      0.80      1113\n",
      "weighted avg       0.85      0.85      0.85      1113\n",
      "\n",
      "Test:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.88      0.83       201\n",
      "           1       0.54      0.36      0.43        78\n",
      "\n",
      "    accuracy                           0.73       279\n",
      "   macro avg       0.66      0.62      0.63       279\n",
      "weighted avg       0.71      0.73      0.72       279\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "===================KNN without PCA Performance=====================\n",
      "95% CI = [ 0.7596334041884075 0.8234103538553504 ]\n",
      "Train:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90       827\n",
      "           1       0.71      0.70      0.70       286\n",
      "\n",
      "    accuracy                           0.85      1113\n",
      "   macro avg       0.80      0.80      0.80      1113\n",
      "weighted avg       0.85      0.85      0.85      1113\n",
      "\n",
      "Test:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.84      0.83       201\n",
      "           1       0.55      0.53      0.54        78\n",
      "\n",
      "    accuracy                           0.75       279\n",
      "   macro avg       0.69      0.68      0.68       279\n",
      "weighted avg       0.75      0.75      0.75       279\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_models(data, 'doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "189e3dc0-f974-4f89-aff5-1193865f830a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\color\\AppData\\Local\\Temp\\ipykernel_21624\\3287934322.py:6: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================Naive Bayes Performance=====================\n",
      "95% CI = [ 0.6712399440273545 0.7373829645955542 ]\n",
      "Train:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.74      0.82       827\n",
      "           1       0.51      0.80      0.62       286\n",
      "\n",
      "    accuracy                           0.75      1113\n",
      "   macro avg       0.71      0.77      0.72      1113\n",
      "weighted avg       0.81      0.75      0.77      1113\n",
      "\n",
      "Test:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.73      0.78       201\n",
      "           1       0.49      0.68      0.57        78\n",
      "\n",
      "    accuracy                           0.71       279\n",
      "   macro avg       0.67      0.70      0.68       279\n",
      "weighted avg       0.75      0.71      0.72       279\n",
      "\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\color\\Anaconda3\\envs\\pr_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\color\\Anaconda3\\envs\\pr_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\color\\Anaconda3\\envs\\pr_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\color\\Anaconda3\\envs\\pr_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\color\\Anaconda3\\envs\\pr_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\color\\Anaconda3\\envs\\pr_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\color\\Anaconda3\\envs\\pr_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\color\\Anaconda3\\envs\\pr_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\color\\Anaconda3\\envs\\pr_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\color\\Anaconda3\\envs\\pr_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\color\\Anaconda3\\envs\\pr_project\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1208: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================Logistic Regression Performance=====================\n",
      "95% CI = [ 0.7388230386006595 0.7903597155820948 ]\n",
      "Train:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.92      0.90       827\n",
      "           1       0.73      0.60      0.66       286\n",
      "\n",
      "    accuracy                           0.84      1113\n",
      "   macro avg       0.80      0.76      0.78      1113\n",
      "weighted avg       0.83      0.84      0.83      1113\n",
      "\n",
      "Test:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.84      0.81       201\n",
      "           1       0.50      0.42      0.46        78\n",
      "\n",
      "    accuracy                           0.72       279\n",
      "   macro avg       0.64      0.63      0.63       279\n",
      "weighted avg       0.71      0.72      0.71       279\n",
      "\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "===================KNN with PCA Performance=====================\n",
      "95% CI = [ 0.6714867926303487 0.7031431944996385 ]\n",
      "Train:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.92      0.88       827\n",
      "           1       0.69      0.55      0.61       286\n",
      "\n",
      "    accuracy                           0.82      1113\n",
      "   macro avg       0.77      0.73      0.75      1113\n",
      "weighted avg       0.81      0.82      0.81      1113\n",
      "\n",
      "Test:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.86      0.82       201\n",
      "           1       0.50      0.37      0.43        78\n",
      "\n",
      "    accuracy                           0.72       279\n",
      "   macro avg       0.64      0.61      0.62       279\n",
      "weighted avg       0.70      0.72      0.71       279\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "===================KNN without PCA Performance=====================\n",
      "95% CI = [ 0.5555956281107718 0.6573065597914164 ]\n",
      "Train:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.94      0.89       827\n",
      "           1       0.76      0.52      0.62       286\n",
      "\n",
      "    accuracy                           0.83      1113\n",
      "   macro avg       0.81      0.73      0.76      1113\n",
      "weighted avg       0.83      0.83      0.82      1113\n",
      "\n",
      "Test:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.92      0.84       201\n",
      "           1       0.59      0.29      0.39        78\n",
      "\n",
      "    accuracy                           0.75       279\n",
      "   macro avg       0.68      0.61      0.62       279\n",
      "weighted avg       0.72      0.75      0.71       279\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_models(data, 'tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0432b251-7ea7-4532-8ab4-49d3fe8ad519",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\color\\AppData\\Local\\Temp\\ipykernel_21624\\3287934322.py:6: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================Naive Bayes Performance=====================\n",
      "95% CI = [ 0.6811504950323417 0.7350995049676583 ]\n",
      "Train:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.79      0.73      1593\n",
      "           1       0.75      0.64      0.69      1607\n",
      "\n",
      "    accuracy                           0.71      3200\n",
      "   macro avg       0.72      0.71      0.71      3200\n",
      "weighted avg       0.72      0.71      0.71      3200\n",
      "\n",
      "Test:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.85      0.75       407\n",
      "           1       0.79      0.57      0.66       393\n",
      "\n",
      "    accuracy                           0.71       800\n",
      "   macro avg       0.73      0.71      0.70       800\n",
      "weighted avg       0.73      0.71      0.71       800\n",
      "\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "===================Logistic Regression Performance=====================\n",
      "95% CI = [ 0.7576572297165232 0.7898427702834767 ]\n",
      "Train:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.77      0.77      1593\n",
      "           1       0.77      0.78      0.78      1607\n",
      "\n",
      "    accuracy                           0.77      3200\n",
      "   macro avg       0.77      0.77      0.77      3200\n",
      "weighted avg       0.77      0.77      0.77      3200\n",
      "\n",
      "Test:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.80      0.79       407\n",
      "           1       0.78      0.77      0.77       393\n",
      "\n",
      "    accuracy                           0.78       800\n",
      "   macro avg       0.78      0.78      0.78       800\n",
      "weighted avg       0.78      0.78      0.78       800\n",
      "\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "===================KNN with PCA Performance=====================\n",
      "95% CI = [ 0.8153385768917236 0.8496614231082762 ]\n",
      "Train:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.76      0.84      1593\n",
      "           1       0.80      0.95      0.87      1607\n",
      "\n",
      "    accuracy                           0.86      3200\n",
      "   macro avg       0.87      0.86      0.86      3200\n",
      "weighted avg       0.87      0.86      0.86      3200\n",
      "\n",
      "Test:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.73      0.78       407\n",
      "           1       0.76      0.86      0.80       393\n",
      "\n",
      "    accuracy                           0.79       800\n",
      "   macro avg       0.80      0.79      0.79       800\n",
      "weighted avg       0.80      0.79      0.79       800\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "===================KNN without PCA Performance=====================\n",
      "95% CI = [ 0.8117412146583319 0.8482587853416678 ]\n",
      "Train:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.79      0.86      1593\n",
      "           1       0.82      0.96      0.89      1607\n",
      "\n",
      "    accuracy                           0.88      3200\n",
      "   macro avg       0.89      0.87      0.87      3200\n",
      "weighted avg       0.89      0.88      0.87      3200\n",
      "\n",
      "Test:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.66      0.72       407\n",
      "           1       0.70      0.82      0.76       393\n",
      "\n",
      "    accuracy                           0.74       800\n",
      "   macro avg       0.75      0.74      0.74       800\n",
      "weighted avg       0.75      0.74      0.74       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_models(equal, 'doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6301686d-4796-41e2-adda-8a063774b16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\color\\AppData\\Local\\Temp\\ipykernel_21624\\3287934322.py:6: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================Naive Bayes Performance=====================\n",
      "95% CI = [ 0.7102528541931672 0.7359971458068328 ]\n",
      "Train:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.70      0.73      1593\n",
      "           1       0.72      0.78      0.75      1607\n",
      "\n",
      "    accuracy                           0.74      3200\n",
      "   macro avg       0.74      0.74      0.74      3200\n",
      "weighted avg       0.74      0.74      0.74      3200\n",
      "\n",
      "Test:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.68      0.71       407\n",
      "           1       0.69      0.75      0.72       393\n",
      "\n",
      "    accuracy                           0.71       800\n",
      "   macro avg       0.72      0.71      0.71       800\n",
      "weighted avg       0.72      0.71      0.71       800\n",
      "\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "===================Logistic Regression Performance=====================\n",
      "95% CI = [ 0.7328868279134311 0.7596131720865691 ]\n",
      "Train:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.72      0.75      1593\n",
      "           1       0.74      0.82      0.78      1607\n",
      "\n",
      "    accuracy                           0.77      3200\n",
      "   macro avg       0.77      0.77      0.77      3200\n",
      "weighted avg       0.77      0.77      0.77      3200\n",
      "\n",
      "Test:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.71      0.74       407\n",
      "           1       0.72      0.78      0.75       393\n",
      "\n",
      "    accuracy                           0.74       800\n",
      "   macro avg       0.74      0.74      0.74       800\n",
      "weighted avg       0.74      0.74      0.74       800\n",
      "\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "===================KNN with PCA Performance=====================\n",
      "95% CI = [ 0.6808813190300842 0.7072436809699159 ]\n",
      "Train:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.71      0.77      1593\n",
      "           1       0.75      0.87      0.81      1607\n",
      "\n",
      "    accuracy                           0.79      3200\n",
      "   macro avg       0.80      0.79      0.79      3200\n",
      "weighted avg       0.80      0.79      0.79      3200\n",
      "\n",
      "Test:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.60      0.66       407\n",
      "           1       0.65      0.77      0.71       393\n",
      "\n",
      "    accuracy                           0.69       800\n",
      "   macro avg       0.69      0.69      0.68       800\n",
      "weighted avg       0.69      0.69      0.68       800\n",
      "\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "===================KNN without PCA Performance=====================\n",
      "95% CI = [ 0.5711206287252333 0.6032543712747669 ]\n",
      "Train:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.49      0.62      1593\n",
      "           1       0.64      0.92      0.76      1607\n",
      "\n",
      "    accuracy                           0.70      3200\n",
      "   macro avg       0.75      0.70      0.69      3200\n",
      "weighted avg       0.75      0.70      0.69      3200\n",
      "\n",
      "Test:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.32      0.44       407\n",
      "           1       0.55      0.86      0.67       393\n",
      "\n",
      "    accuracy                           0.58       800\n",
      "   macro avg       0.63      0.59      0.55       800\n",
      "weighted avg       0.63      0.58      0.55       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_models(equal, 'tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20247209-ac3e-4f13-9ca2-154a830e48ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
