{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5db0976e-422a-4861-be36-0d77394a2346",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/alexacole/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alexacole/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/alexacole/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "import sklearn\n",
    "#import spacy\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75b5a047-4688-47da-af77-705569e87c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['source', 'id', 'text'],\n",
       "        num_rows: 1392522\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset('artem9k/ai-text-detection-pile')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a85aadf-da6e-49da-a4a7-9987e49b454c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>human</td>\n",
       "      <td>0</td>\n",
       "      <td>12 Years a Slave: An Analysis of the Film Essa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>human</td>\n",
       "      <td>1</td>\n",
       "      <td>20+ Social Media Post Ideas to Radically Simpl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>human</td>\n",
       "      <td>2</td>\n",
       "      <td>2022 Russian Invasion of Ukraine in Global Med...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>human</td>\n",
       "      <td>3</td>\n",
       "      <td>533 U.S. 27 (2001) Kyllo v. United States: The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>human</td>\n",
       "      <td>4</td>\n",
       "      <td>A Charles Schwab Corporation Case Essay\\n\\nCha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source  id                                               text\n",
       "0  human   0  12 Years a Slave: An Analysis of the Film Essa...\n",
       "1  human   1  20+ Social Media Post Ideas to Radically Simpl...\n",
       "2  human   2  2022 Russian Invasion of Ukraine in Global Med...\n",
       "3  human   3  533 U.S. 27 (2001) Kyllo v. United States: The...\n",
       "4  human   4  A Charles Schwab Corporation Case Essay\\n\\nCha..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(dataset['train'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba0f467",
   "metadata": {},
   "source": [
    "## Reformat Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bc4493a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['human', 'ai'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['source'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82fa5be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['source'] = [1 if x == 'ai' else 0 for x in df['source']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bbe1c22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12 Years a Slave: An Analysis of the Film Essa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20+ Social Media Post Ideas to Radically Simpl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2022 Russian Invasion of Ukraine in Global Med...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>533 U.S. 27 (2001) Kyllo v. United States: The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>A Charles Schwab Corporation Case Essay\\n\\nCha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source  id                                               text\n",
       "0       0   0  12 Years a Slave: An Analysis of the Film Essa...\n",
       "1       0   1  20+ Social Media Post Ideas to Radically Simpl...\n",
       "2       0   2  2022 Russian Invasion of Ukraine in Global Med...\n",
       "3       0   3  533 U.S. 27 (2001) Kyllo v. United States: The...\n",
       "4       0   4  A Charles Schwab Corporation Case Essay\\n\\nCha..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc858b14-9dbe-470f-928f-90881e776f13",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32b518de-439b-4bcc-93e2-534898f17a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for preprocessing\n",
    "def remove_urls(text):\n",
    "    return re.sub(r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\", \" \", text) # regex taken from https://www.geeksforgeeks.org/python-check-url-string/\n",
    "\n",
    "def remove_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def remove_extra_whitespace(text):\n",
    "    text = text.strip()\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def lemmatizer(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    l = nltk.stem.WordNetLemmatizer()\n",
    "    tokens = [l.lemmatize(token) for token in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# eliminate punctuation\n",
    "def remove_punctuation(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def tokenize_pre_process(text): # for preprocessing using this link: https://spotintelligence.com/2022/12/21/nltk-preprocessing-pipeline/\n",
    "    # tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # remove stop words\n",
    "    stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "\n",
    "    # remove top 10% most frequent words \n",
    "    fdist = nltk.FreqDist(tokens)\n",
    "    tokens = [token for token in tokens if fdist[token] < fdist.N() * 0.1]\n",
    "\n",
    "    # stemming\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    # eliminate punctuation\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8857fa40-58c8-4c7b-a125-075104c50009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # encoding to ascii\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    \n",
    "    # convert text to lower case\n",
    "    text = text.lower()\n",
    "\n",
    "    # remove html tags \n",
    "    text = remove_html(text)\n",
    "\n",
    "    # remove urls \n",
    "    text = remove_urls(text)\n",
    "\n",
    "    # remove extra whitespace\n",
    "    text = remove_extra_whitespace(text)\n",
    "\n",
    "    # remove stop words\n",
    "    text = remove_stop_words(text)\n",
    "    \n",
    "    # remove punctuation\n",
    "    text = remove_punctuation(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1afd640-75a0-4bae-95ce-bc0a5a4b8360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text2(text):\n",
    "    # encoding to ascii\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    \n",
    "    # convert text to lower case\n",
    "    text = text.lower()\n",
    "\n",
    "    # remove html tags \n",
    "    text = remove_html(text)\n",
    "\n",
    "    # remove urls \n",
    "    text = remove_urls(text)\n",
    "\n",
    "    # remove extra whitespace\n",
    "    text = remove_extra_whitespace(text)\n",
    "\n",
    "    # remove stop words\n",
    "    text = remove_stop_words(text)\n",
    "    \n",
    "    # remove punctuation\n",
    "    text = remove_punctuation(text)\n",
    "\n",
    "    # lemmatize words\n",
    "    text = lemmatizer(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343436b3-c476-4b04-babb-29588e7cb236",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b5910b-2c22-4289-9b51-334e25c0fb26",
   "metadata": {},
   "source": [
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "294c2ed8-1f0f-4bca-ab6e-5392c1e64b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12b21e42-cb07-4f10-ab2a-c677818219f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(max_df=0.9,min_df=0.1)\n",
    "X = vec.fit_transform(df.text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69e51d74-f9c4-47b5-ac77-9995c56eafea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['10', '11', '12', '15', '19', '2017', '2018', '2019', '2020',\n",
       "       '2021', '2022', 'ability', 'able', 'about', 'above', 'access',\n",
       "       'according', 'achieve', 'across', 'act', 'action', 'actions',\n",
       "       'activities', 'activity', 'addition', 'additional', 'additionally',\n",
       "       'address', 'affect', 'affected', 'affects', 'after', 'against',\n",
       "       'age', 'al', 'all', 'allow', 'allowed', 'allows', 'almost',\n",
       "       'already', 'also', 'although', 'always', 'america', 'american',\n",
       "       'americans', 'among', 'an', 'analysis', 'another', 'any',\n",
       "       'approach', 'approaches', 'appropriate', 'are', 'area', 'areas',\n",
       "       'around', 'article', 'aspect', 'aspects', 'associated', 'at',\n",
       "       'attention', 'author', 'available', 'avoid', 'back', 'based',\n",
       "       'basis', 'be', 'became', 'because', 'become', 'becomes', 'been',\n",
       "       'before', 'behavior', 'being', 'believe', 'benefits', 'best',\n",
       "       'better', 'between', 'black', 'body', 'both', 'business', 'but',\n",
       "       'by', 'can', 'cannot', 'care', 'case', 'cases', 'cause', 'caused',\n",
       "       'causes', 'central', 'century', 'certain', 'challenges', 'change',\n",
       "       'changes', 'characteristics', 'children', 'cited', 'citizens',\n",
       "       'clear', 'close', 'come', 'common', 'communication', 'community',\n",
       "       'companies', 'company', 'compared', 'complex', 'concept',\n",
       "       'concerns', 'conclusion', 'condition', 'conditions',\n",
       "       'consequences', 'consider', 'considered', 'considering',\n",
       "       'contents', 'context', 'contribute', 'control', 'cost', 'costs',\n",
       "       'could', 'countries', 'country', 'covid', 'create', 'created',\n",
       "       'creating', 'critical', 'crucial', 'cultural', 'culture',\n",
       "       'current', 'data', 'day', 'decision', 'decisions', 'despite',\n",
       "       'determine', 'develop', 'developed', 'developing', 'development',\n",
       "       'did', 'differences', 'different', 'difficult', 'direct',\n",
       "       'directly', 'discussion', 'disease', 'do', 'does', 'due', 'during',\n",
       "       'each', 'early', 'economic', 'education', 'effect', 'effective',\n",
       "       'effectively', 'effects', 'elements', 'employees', 'end', 'enough',\n",
       "       'ensure', 'environment', 'especially', 'essay', 'essential',\n",
       "       'established', 'et', 'even', 'events', 'every', 'everyone',\n",
       "       'evidence', 'example', 'existing', 'experience', 'face', 'fact',\n",
       "       'factor', 'factors', 'family', 'features', 'feel', 'few', 'field',\n",
       "       'finally', 'financial', 'find', 'first', 'five', 'focus', 'follow',\n",
       "       'following', 'food', 'form', 'forms', 'found', 'framework', 'free',\n",
       "       'from', 'full', 'further', 'furthermore', 'future', 'general',\n",
       "       'get', 'give', 'given', 'global', 'go', 'goal', 'goals', 'good',\n",
       "       'government', 'great', 'group', 'groups', 'growth', 'had', 'hand',\n",
       "       'has', 'have', 'having', 'he', 'health', 'healthcare', 'help',\n",
       "       'helps', 'hence', 'her', 'high', 'higher', 'highly', 'him', 'his',\n",
       "       'history', 'home', 'how', 'however', 'human', 'idea', 'ideas',\n",
       "       'identify', 'if', 'impact', 'importance', 'important', 'improve',\n",
       "       'include', 'includes', 'including', 'increase', 'increased',\n",
       "       'increasing', 'individual', 'individuals', 'industry', 'influence',\n",
       "       'information', 'instance', 'instead', 'interest', 'international',\n",
       "       'into', 'introduction', 'involved', 'issue', 'issues', 'its',\n",
       "       'itself', 'journal', 'just', 'key', 'know', 'knowledge', 'known',\n",
       "       'lack', 'large', 'last', 'later', 'law', 'lead', 'leading',\n",
       "       'leads', 'learning', 'led', 'legal', 'less', 'level', 'levels',\n",
       "       'life', 'like', 'likely', 'limited', 'live', 'lives', 'living',\n",
       "       'local', 'long', 'low', 'lower', 'made', 'main', 'maintain',\n",
       "       'major', 'make', 'makes', 'making', 'man', 'management', 'many',\n",
       "       'market', 'matter', 'may', 'me', 'meaning', 'means', 'measures',\n",
       "       'media', 'medical', 'members', 'mental', 'mentioned', 'method',\n",
       "       'methods', 'might', 'model', 'modern', 'money', 'more', 'moreover',\n",
       "       'most', 'much', 'multiple', 'must', 'my', 'national', 'natural',\n",
       "       'nature', 'necessary', 'need', 'needed', 'needs', 'negative',\n",
       "       'never', 'new', 'next', 'no', 'non', 'not', 'now', 'number',\n",
       "       'numerous', 'often', 'one', 'ones', 'only', 'operations',\n",
       "       'opinion', 'opportunities', 'opportunity', 'or', 'order',\n",
       "       'organization', 'organizations', 'other', 'others', 'our', 'out',\n",
       "       'outcomes', 'over', 'overall', 'own', 'pandemic', 'paper', 'part',\n",
       "       'particular', 'particularly', 'past', 'patient', 'patients',\n",
       "       'people', 'performance', 'period', 'person', 'personal',\n",
       "       'perspective', 'physical', 'place', 'plan', 'play', 'point',\n",
       "       'policy', 'political', 'population', 'position', 'positive',\n",
       "       'possible', 'potential', 'power', 'pp', 'practice', 'practices',\n",
       "       'present', 'presented', 'prevent', 'primary', 'principles',\n",
       "       'problem', 'problems', 'process', 'processes', 'product',\n",
       "       'production', 'products', 'professional', 'promote', 'proper',\n",
       "       'provide', 'provided', 'provides', 'providing', 'public',\n",
       "       'purpose', 'quality', 'question', 'range', 'rate', 'rather',\n",
       "       'real', 'reason', 'reasons', 'receive', 'reduce', 'reference',\n",
       "       'references', 'regarding', 'related', 'relationship',\n",
       "       'relationships', 'relevant', 'report', 'require', 'required',\n",
       "       'requires', 'research', 'resources', 'responsibility',\n",
       "       'responsible', 'result', 'results', 'review', 'right', 'rights',\n",
       "       'risk', 'risks', 'role', 'safety', 'same', 'science', 'second',\n",
       "       'security', 'see', 'seen', 'self', 'service', 'services', 'set',\n",
       "       'several', 'severe', 'share', 'she', 'should', 'show', 'shows',\n",
       "       'significant', 'significantly', 'similar', 'since', 'situation',\n",
       "       'skills', 'small', 'so', 'social', 'society', 'some', 'source',\n",
       "       'sources', 'specific', 'spread', 'state', 'states', 'status',\n",
       "       'still', 'story', 'strategies', 'strategy', 'strong', 'structure',\n",
       "       'studies', 'study', 'subject', 'success', 'successful', 'such',\n",
       "       'support', 'system', 'systems', 'table', 'take', 'taken', 'taking',\n",
       "       'technology', 'term', 'terms', 'than', 'their', 'them',\n",
       "       'themselves', 'then', 'theory', 'there', 'therefore', 'these',\n",
       "       'they', 'third', 'this', 'those', 'though', 'three', 'through',\n",
       "       'throughout', 'thus', 'time', 'times', 'today', 'together',\n",
       "       'topic', 'towards', 'treatment', 'turn', 'two', 'type', 'types',\n",
       "       'under', 'understand', 'understanding', 'unique', 'united',\n",
       "       'university', 'up', 'us', 'use', 'used', 'uses', 'using',\n",
       "       'usually', 'value', 'values', 'various', 'very', 'view', 'vital',\n",
       "       'want', 'was', 'way', 'ways', 'we', 'web', 'well', 'were', 'what',\n",
       "       'when', 'where', 'whether', 'which', 'while', 'white', 'who',\n",
       "       'whole', 'why', 'will', 'within', 'without', 'women', 'words',\n",
       "       'work', 'workers', 'working', 'works', 'world', 'would', 'year',\n",
       "       'years', 'you', 'young'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f22d6b9-f08c-4ac8-a8fa-6598f213a5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec2 = CountVectorizer(preprocessor=preprocess_text,max_df=0.9,min_df=0.1)\n",
    "X2 = vec2.fit_transform(df.text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be6ff840-a8e5-4cfe-86ec-2eefd27edc34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['10', '11', '12', '15', '19', '2017', '2018', '2019', '2020',\n",
       "       '2021', '2022', 'ability', 'able', 'access', 'according',\n",
       "       'achieve', 'across', 'act', 'action', 'actions', 'activities',\n",
       "       'activity', 'addition', 'additional', 'additionally', 'address',\n",
       "       'affect', 'affected', 'affects', 'age', 'al', 'allow', 'allowed',\n",
       "       'allows', 'almost', 'already', 'also', 'although', 'always',\n",
       "       'america', 'american', 'americans', 'among', 'analysis', 'another',\n",
       "       'approach', 'approaches', 'appropriate', 'area', 'areas', 'around',\n",
       "       'article', 'aspect', 'aspects', 'associated', 'attention',\n",
       "       'author', 'authors', 'available', 'avoid', 'back', 'based',\n",
       "       'basis', 'became', 'become', 'becomes', 'behavior', 'believe',\n",
       "       'benefits', 'best', 'better', 'body', 'business', 'care', 'case',\n",
       "       'cases', 'cause', 'caused', 'causes', 'central', 'century',\n",
       "       'certain', 'challenges', 'change', 'changes', 'characteristics',\n",
       "       'children', 'cited', 'citizens', 'clear', 'close', 'come',\n",
       "       'common', 'communication', 'community', 'companies', 'company',\n",
       "       'companys', 'compared', 'complex', 'concept', 'concerns',\n",
       "       'conclusion', 'condition', 'conditions', 'consequences',\n",
       "       'consider', 'considered', 'considering', 'contents', 'context',\n",
       "       'contribute', 'control', 'cost', 'costs', 'could', 'countries',\n",
       "       'country', 'covid', 'create', 'created', 'creating', 'critical',\n",
       "       'crucial', 'cultural', 'culture', 'current', 'data', 'day',\n",
       "       'decision', 'decisions', 'despite', 'determine', 'develop',\n",
       "       'developed', 'developing', 'development', 'differences',\n",
       "       'different', 'difficult', 'direct', 'directly', 'discussion',\n",
       "       'disease', 'due', 'early', 'economic', 'education', 'effect',\n",
       "       'effective', 'effectively', 'effects', 'elements', 'employees',\n",
       "       'end', 'enough', 'ensure', 'environment', 'especially', 'essay',\n",
       "       'essential', 'established', 'et', 'even', 'events', 'every',\n",
       "       'evidence', 'example', 'existing', 'experience', 'face', 'fact',\n",
       "       'factor', 'factors', 'family', 'features', 'feel', 'field',\n",
       "       'finally', 'financial', 'find', 'first', 'five', 'focus', 'follow',\n",
       "       'following', 'food', 'form', 'forms', 'found', 'framework', 'free',\n",
       "       'full', 'furthermore', 'future', 'general', 'get', 'give', 'given',\n",
       "       'global', 'go', 'goal', 'goals', 'good', 'government', 'great',\n",
       "       'group', 'groups', 'growth', 'hand', 'health', 'healthcare',\n",
       "       'help', 'helps', 'hence', 'high', 'higher', 'highly', 'history',\n",
       "       'home', 'however', 'human', 'idea', 'ideas', 'identify', 'impact',\n",
       "       'importance', 'important', 'improve', 'include', 'includes',\n",
       "       'including', 'increase', 'increased', 'increasing', 'individual',\n",
       "       'individuals', 'industry', 'influence', 'information', 'instance',\n",
       "       'instead', 'interest', 'international', 'introduction', 'involved',\n",
       "       'issue', 'issues', 'journal', 'key', 'know', 'knowledge', 'known',\n",
       "       'lack', 'large', 'last', 'later', 'law', 'lead', 'leading',\n",
       "       'leads', 'learning', 'led', 'legal', 'less', 'level', 'levels',\n",
       "       'life', 'like', 'likely', 'limited', 'live', 'lives', 'living',\n",
       "       'local', 'long', 'low', 'lower', 'made', 'main', 'maintain',\n",
       "       'major', 'make', 'makes', 'making', 'management', 'many', 'market',\n",
       "       'matter', 'may', 'meaning', 'means', 'measures', 'media',\n",
       "       'medical', 'members', 'mental', 'mentioned', 'method', 'methods',\n",
       "       'might', 'model', 'modern', 'money', 'moreover', 'much',\n",
       "       'multiple', 'must', 'national', 'natural', 'nature', 'necessary',\n",
       "       'need', 'needed', 'needs', 'negative', 'never', 'new', 'next',\n",
       "       'non', 'number', 'numerous', 'often', 'one', 'ones', 'operations',\n",
       "       'opinion', 'opportunities', 'opportunity', 'order', 'organization',\n",
       "       'organizations', 'others', 'outcomes', 'overall', 'pandemic',\n",
       "       'paper', 'part', 'particular', 'particularly', 'past', 'patient',\n",
       "       'patients', 'people', 'peoples', 'performance', 'period', 'person',\n",
       "       'personal', 'persons', 'perspective', 'physical', 'place', 'plan',\n",
       "       'play', 'point', 'policy', 'political', 'population', 'position',\n",
       "       'positive', 'possible', 'potential', 'power', 'pp', 'practice',\n",
       "       'practices', 'present', 'presented', 'prevent', 'primary',\n",
       "       'principles', 'problem', 'problems', 'process', 'processes',\n",
       "       'product', 'production', 'products', 'professional', 'programs',\n",
       "       'promote', 'proper', 'provide', 'provided', 'provides',\n",
       "       'providing', 'public', 'purpose', 'quality', 'question', 'range',\n",
       "       'rate', 'rather', 'real', 'reason', 'reasons', 'receive', 'reduce',\n",
       "       'reference', 'references', 'regarding', 'related', 'relationship',\n",
       "       'relationships', 'relevant', 'report', 'require', 'required',\n",
       "       'requires', 'research', 'resources', 'responsibility',\n",
       "       'responsible', 'result', 'results', 'review', 'right', 'rights',\n",
       "       'risk', 'risks', 'role', 'safety', 'science', 'second', 'security',\n",
       "       'see', 'seen', 'self', 'service', 'services', 'set', 'several',\n",
       "       'severe', 'share', 'show', 'shows', 'significant', 'significantly',\n",
       "       'similar', 'since', 'situation', 'skills', 'small', 'social',\n",
       "       'society', 'source', 'sources', 'specific', 'spread', 'state',\n",
       "       'states', 'status', 'still', 'strategies', 'strategy', 'strong',\n",
       "       'structure', 'studies', 'study', 'subject', 'success',\n",
       "       'successful', 'support', 'system', 'systems', 'table', 'take',\n",
       "       'taken', 'taking', 'technology', 'term', 'terms', 'theory',\n",
       "       'therefore', 'third', 'though', 'three', 'throughout', 'thus',\n",
       "       'time', 'times', 'together', 'topic', 'towards', 'treatment',\n",
       "       'turn', 'two', 'type', 'types', 'understand', 'understanding',\n",
       "       'unique', 'united', 'university', 'us', 'use', 'used', 'uses',\n",
       "       'using', 'usually', 'value', 'values', 'various', 'view', 'vital',\n",
       "       'want', 'way', 'ways', 'web', 'well', 'whether', 'white', 'whole',\n",
       "       'within', 'without', 'women', 'words', 'work', 'workers',\n",
       "       'working', 'works', 'world', 'would', 'year', 'years', 'young'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec2.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "babe2fe5-7e00-49a3-928d-4dae81182844",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec3 = CountVectorizer(preprocessor=preprocess_text2,max_df=0.9,min_df=0.1)\n",
    "X3 = vec3.fit_transform(df.text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4165a5c5-cea7-42fc-9d2a-fd5f1e413640",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['10', '11', '12', '15', '19', '2017', '2018', '2019', '2020',\n",
       "       '2021', '2022', 'ability', 'able', 'access', 'according',\n",
       "       'account', 'achieve', 'across', 'act', 'action', 'activity',\n",
       "       'addition', 'additional', 'additionally', 'address', 'advantage',\n",
       "       'affect', 'affected', 'age', 'aim', 'al', 'allow', 'allowed',\n",
       "       'allows', 'almost', 'already', 'also', 'although', 'always',\n",
       "       'america', 'american', 'among', 'amount', 'analysis', 'another',\n",
       "       'application', 'approach', 'appropriate', 'area', 'around',\n",
       "       'article', 'aspect', 'assessment', 'associated', 'attention',\n",
       "       'attitude', 'author', 'authority', 'available', 'avoid', 'back',\n",
       "       'background', 'based', 'basis', 'became', 'become', 'becomes',\n",
       "       'behavior', 'being', 'belief', 'believe', 'benefit', 'best',\n",
       "       'better', 'black', 'body', 'book', 'business', 'care', 'case',\n",
       "       'cause', 'caused', 'center', 'central', 'century', 'certain',\n",
       "       'challenge', 'chance', 'change', 'character', 'characteristic',\n",
       "       'child', 'choice', 'cited', 'citizen', 'claim', 'clear', 'close',\n",
       "       'come', 'common', 'communication', 'community', 'company',\n",
       "       'compared', 'complex', 'component', 'concept', 'concern',\n",
       "       'conclusion', 'condition', 'conduct', 'conflict', 'connection',\n",
       "       'consequence', 'consider', 'consideration', 'considered',\n",
       "       'considering', 'content', 'context', 'contribute', 'control',\n",
       "       'cost', 'could', 'country', 'covid', 'create', 'created',\n",
       "       'creating', 'critical', 'crucial', 'cultural', 'culture',\n",
       "       'current', 'customer', 'data', 'day', 'death', 'decision',\n",
       "       'demand', 'desire', 'despite', 'detail', 'determine', 'develop',\n",
       "       'developed', 'developing', 'development', 'difference',\n",
       "       'different', 'difficult', 'direct', 'directly', 'discussion',\n",
       "       'disease', 'due', 'early', 'economic', 'ed', 'education', 'effect',\n",
       "       'effective', 'effectively', 'effort', 'element', 'employee', 'end',\n",
       "       'enough', 'ensure', 'environment', 'especially', 'essay',\n",
       "       'essential', 'established', 'et', 'even', 'event', 'every',\n",
       "       'evidence', 'example', 'existing', 'experience', 'face', 'fact',\n",
       "       'factor', 'family', 'feature', 'feel', 'feeling', 'field',\n",
       "       'finally', 'financial', 'find', 'finding', 'first', 'five',\n",
       "       'focus', 'follow', 'following', 'food', 'force', 'form', 'found',\n",
       "       'framework', 'free', 'full', 'function', 'furthermore', 'future',\n",
       "       'general', 'get', 'give', 'given', 'global', 'go', 'goal', 'good',\n",
       "       'government', 'great', 'group', 'growth', 'hand', 'health',\n",
       "       'healthcare', 'help', 'hence', 'high', 'higher', 'highly',\n",
       "       'history', 'home', 'however', 'human', 'idea', 'identify', 'image',\n",
       "       'impact', 'importance', 'important', 'improve', 'improvement',\n",
       "       'include', 'includes', 'including', 'income', 'increase',\n",
       "       'increased', 'increasing', 'individual', 'industry', 'influence',\n",
       "       'information', 'instance', 'instead', 'interaction', 'interest',\n",
       "       'international', 'introduction', 'involved', 'issue', 'job',\n",
       "       'journal', 'keep', 'key', 'know', 'knowledge', 'known', 'lack',\n",
       "       'large', 'last', 'later', 'law', 'lead', 'leader', 'leading',\n",
       "       'learning', 'led', 'legal', 'less', 'level', 'life', 'like',\n",
       "       'likely', 'limited', 'live', 'living', 'local', 'long', 'look',\n",
       "       'loss', 'low', 'lower', 'made', 'main', 'maintain', 'major',\n",
       "       'make', 'making', 'man', 'management', 'many', 'market',\n",
       "       'material', 'matter', 'may', 'mean', 'meaning', 'measure',\n",
       "       'medical', 'medicine', 'medium', 'meet', 'member', 'mental',\n",
       "       'mentioned', 'method', 'might', 'million', 'model', 'modern',\n",
       "       'money', 'moreover', 'movement', 'much', 'multiple', 'must',\n",
       "       'national', 'natural', 'nature', 'necessary', 'need', 'needed',\n",
       "       'negative', 'never', 'new', 'next', 'non', 'note', 'number',\n",
       "       'numerous', 'objective', 'offer', 'often', 'one', 'open',\n",
       "       'operation', 'opinion', 'opportunity', 'order', 'organization',\n",
       "       'others', 'outcome', 'overall', 'pandemic', 'paper', 'part',\n",
       "       'particular', 'particularly', 'past', 'patient', 'pay', 'people',\n",
       "       'perception', 'performance', 'period', 'person', 'personal',\n",
       "       'perspective', 'physical', 'place', 'plan', 'play', 'point',\n",
       "       'policy', 'political', 'population', 'position', 'positive',\n",
       "       'possibility', 'possible', 'potential', 'power', 'pp', 'practice',\n",
       "       'present', 'presented', 'press', 'prevent', 'primary', 'principle',\n",
       "       'problem', 'process', 'product', 'production', 'professional',\n",
       "       'program', 'project', 'promote', 'proper', 'provide', 'provided',\n",
       "       'provides', 'providing', 'public', 'purpose', 'put', 'quality',\n",
       "       'question', 'range', 'rate', 'rather', 'real', 'reason', 'receive',\n",
       "       'reduce', 'reference', 'regarding', 'related', 'relation',\n",
       "       'relationship', 'relevant', 'report', 'require', 'required',\n",
       "       'requires', 'research', 'researcher', 'resource', 'response',\n",
       "       'responsibility', 'responsible', 'result', 'review', 'right',\n",
       "       'risk', 'role', 'rule', 'safety', 'say', 'science', 'second',\n",
       "       'security', 'see', 'seen', 'self', 'service', 'set', 'setting',\n",
       "       'several', 'severe', 'share', 'show', 'side', 'significant',\n",
       "       'significantly', 'similar', 'since', 'situation', 'skill', 'small',\n",
       "       'social', 'society', 'solution', 'source', 'specific', 'spread',\n",
       "       'stage', 'standard', 'start', 'state', 'statement', 'status',\n",
       "       'step', 'still', 'story', 'strategy', 'strong', 'structure',\n",
       "       'study', 'subject', 'success', 'successful', 'support', 'system',\n",
       "       'table', 'take', 'taken', 'taking', 'task', 'technology', 'term',\n",
       "       'theory', 'therefore', 'thing', 'think', 'third', 'though',\n",
       "       'thought', 'threat', 'three', 'throughout', 'thus', 'time',\n",
       "       'today', 'together', 'tool', 'topic', 'towards', 'treatment',\n",
       "       'trend', 'turn', 'two', 'type', 'understand', 'understanding',\n",
       "       'unique', 'united', 'university', 'us', 'use', 'used', 'using',\n",
       "       'usually', 'value', 'various', 'view', 'vital', 'want', 'way',\n",
       "       'web', 'well', 'whether', 'white', 'whole', 'within', 'without',\n",
       "       'woman', 'word', 'work', 'worker', 'working', 'world', 'would',\n",
       "       'writing', 'year', 'young'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec3.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7aade4-3ad6-42a3-9231-dda946e1e1bd",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b505b72-5113-4ebc-945d-17283978efdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b174821b-c9cf-4838-8f0c-5a1e306ec266",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(preprocessor=preprocess_text2,max_df=0.9,min_df=0.1) \n",
    "tfidf_matrix = tfidf.fit_transform(df.text[:1000]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59f8c686-d2e5-4475-a506-2270f0e26f57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1000x546 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 104485 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc479d21-401e-4edc-ac47-74bb09d52391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.05350602, 0.37865824, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.13928897, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.05615849, 0.        , 0.10848852,\n",
       "        0.        , 0.07308116, 0.0830871 , 0.        , 0.        ,\n",
       "        0.15936191, 0.        , 0.        , 0.03771018, 0.03654908,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.05321945, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.06589735, 0.        , 0.1026759 ,\n",
       "        0.04175984, 0.        , 0.        , 0.        , 0.05198634,\n",
       "        0.05133795, 0.        , 0.        , 0.        , 0.22263763,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.05211903,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.16183536,\n",
       "        0.        , 0.0947624 , 0.        , 0.        , 0.05321945,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.04459503,\n",
       "        0.        , 0.04227545, 0.03926676, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.07462859, 0.04016775,\n",
       "        0.        , 0.04630602, 0.        , 0.        , 0.03889473,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.03231159,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.02108687, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.03214565,\n",
       "        0.        , 0.        , 0.        , 0.08020359, 0.        ,\n",
       "        0.17216071, 0.        , 0.        , 0.05533463, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.21064146,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.04359666, 0.        , 0.        , 0.        , 0.12293953,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.03252132,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.05293762, 0.10565794, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.10233805, 0.        ,\n",
       "        0.        , 0.        , 0.04319733, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.10002801,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.06159351, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.05266036, 0.        , 0.04063676, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.08237811, 0.11313055, 0.05533463,\n",
       "        0.        , 0.        , 0.        , 0.22263763, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.16504774, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.16600388, 0.12288254,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.02204115, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.0473812 , 0.        , 0.        ,\n",
       "        0.03612675, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.14886514, 0.05293762, 0.        , 0.        ,\n",
       "        0.0378827 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.05307795, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.10909749, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.05409403, 0.        , 0.        , 0.        ,\n",
       "        0.02255085, 0.        , 0.        , 0.        , 0.04459503,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.04125937, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.05615849, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.06659688, 0.        , 0.        , 0.05198634,\n",
       "        0.03411268, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.05266036, 0.06659688,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.15439701,\n",
       "        0.        , 0.        , 0.04952877, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.03612675, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.04450956,\n",
       "        0.        , 0.        , 0.05415229, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.18089486, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.04494125, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.04056896, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.27829704, 0.        , 0.        ,\n",
       "        0.        , 0.04728061, 0.        , 0.        , 0.        ,\n",
       "        0.04758419, 0.        , 0.03581673, 0.        , 0.36602611,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix[0].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6605d305-f777-4231-8df9-421e44f51fbe",
   "metadata": {},
   "source": [
    "### word2vec\n",
    "#### skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "51094dc5-80b8-4e41-a6a0-a09458427733",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3c3851dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract training samples and tokenize\n",
    "training_samples = df['text'][:1000].apply(preprocess_text2)\n",
    "training_samples = training_samples.apply(nltk.tokenize.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e322197c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [12, year, slave, analysis, film, essay, 2013,...\n",
       "1      [20+, social, medium, post, idea, radically, s...\n",
       "2      [2022, russian, invasion, ukraine, global, med...\n",
       "3      [533, u.s., 27, 2001, kyllo, v., united, state...\n",
       "4      [charles, schwab, corporation, case, essay, ch...\n",
       "                             ...                        \n",
       "995    [working, condition, lead, stress, amazon, rep...\n",
       "996    [world, medical, relief, covid-19, essay, thro...\n",
       "997    [world, war, two, ramification, essay, article...\n",
       "998    [wyeth, christinas, world, vs., vermeer, girl,...\n",
       "999    [yelling, past, modern, society, essay, yellin...\n",
       "Name: text, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ea8e427b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Word2Vec(sentences=training_samples, min_count=1,\n",
    "                                vector_size=200, window=5,sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4f0ac1eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x4a731e480>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "16e4362d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [0.074196376, -0.04712757, -0.10678333, 0.1568...\n",
       "1      [-0.04162554, 0.06468972, 0.009438879, 0.20073...\n",
       "2      [0.044293582, 0.016017785, -0.06704636, 0.1293...\n",
       "3      [0.0015910462, -0.004494369, -0.0040816143, 0....\n",
       "4      [0.0060147205, 0.07250863, 0.023821475, 0.1515...\n",
       "                             ...                        \n",
       "995    [0.005436398, 0.060252234, 0.030332573, 0.1491...\n",
       "996    [-0.015175242, 0.08629958, 0.069801085, 0.0873...\n",
       "997    [0.025076255, -0.04014592, -0.06444205, 0.1374...\n",
       "998    [0.042935327, 0.030128071, -0.07588789, 0.1952...\n",
       "999    [0.024119573, -0.01209988, -0.065730825, 0.167...\n",
       "Name: text, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_fe = df['text'][:1000].apply(lambda text: np.mean([model1.wv[word] for word in text.split() if word in model1.wv],axis=0))\n",
    "samples_fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f4961133",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.07419638, -0.04712757, -0.10678333,  0.15683906,  0.14515431,\n",
       "       -0.15465489, -0.19176541,  0.19107528,  0.03810641,  0.22612399,\n",
       "       -0.03611267, -0.01476813, -0.16460265,  0.15785536,  0.19540825,\n",
       "       -0.05912674, -0.24772593, -0.17100364,  0.02698568, -0.22694315,\n",
       "        0.2668589 , -0.125625  ,  0.13611795,  0.10536104,  0.00106118,\n",
       "       -0.0208538 , -0.07283028,  0.07917844, -0.1586108 ,  0.19942044,\n",
       "        0.04520908, -0.0897809 , -0.11267109, -0.13570283,  0.13350329,\n",
       "        0.02113811,  0.22162707,  0.08495472, -0.28973347, -0.0187931 ,\n",
       "        0.03671626, -0.14193617, -0.12827893,  0.20147258,  0.10764107,\n",
       "        0.01140182, -0.05461885, -0.03677811,  0.25168172,  0.11662263,\n",
       "        0.04228579, -0.11505129, -0.15769735,  0.0525364 , -0.062918  ,\n",
       "        0.16133639, -0.00311779, -0.06461628, -0.40714705,  0.05496838,\n",
       "       -0.18862246,  0.01941187, -0.11302729,  0.06715795,  0.23720168,\n",
       "        0.07952263,  0.03036729,  0.15454282, -0.14814423,  0.09918693,\n",
       "       -0.03130501, -0.02819682,  0.26376063, -0.12868635,  0.01025718,\n",
       "        0.04983989,  0.18557775, -0.09595734, -0.26051438, -0.16170502,\n",
       "       -0.19394372, -0.1134695 , -0.04566758,  0.23490359, -0.0441397 ,\n",
       "       -0.00089125, -0.00729477,  0.12772633, -0.0367836 , -0.10344169,\n",
       "        0.1311496 ,  0.15068164,  0.10437188,  0.19011888,  0.12544812,\n",
       "        0.22226329,  0.03433698, -0.04013117,  0.02429733,  0.08647699,\n",
       "       -0.14393581,  0.19884036,  0.00782014, -0.15489921, -0.23076491,\n",
       "       -0.05412977,  0.07686228,  0.10672935,  0.00621271, -0.14381231,\n",
       "       -0.17772263,  0.03959859, -0.01921461,  0.13031717,  0.09566741,\n",
       "       -0.12548897,  0.12078389, -0.1502458 ,  0.01887191,  0.00353346,\n",
       "       -0.06508861, -0.00247155,  0.06066262,  0.00449058,  0.0866553 ,\n",
       "        0.13863221,  0.02648155, -0.03231571,  0.14964914, -0.20801157,\n",
       "        0.08217122,  0.01027454, -0.18742396, -0.19343048, -0.02542618,\n",
       "       -0.00355161,  0.03295797, -0.06745904, -0.20369041, -0.17963022,\n",
       "        0.242029  , -0.07519788, -0.10592262, -0.19704457, -0.04101808,\n",
       "       -0.03647946, -0.10189886,  0.0914493 , -0.13189805,  0.05405809,\n",
       "       -0.03184845, -0.12184805,  0.10512514,  0.02151012, -0.03232336,\n",
       "       -0.00548866,  0.04440384,  0.09461746, -0.15661608,  0.06804425,\n",
       "        0.06380017,  0.04725791, -0.20831512, -0.00551417,  0.04428517,\n",
       "       -0.06406023,  0.04582943, -0.3640777 ,  0.0056918 ,  0.25793257,\n",
       "        0.01076429,  0.05719042,  0.03520456, -0.01819388, -0.08088137,\n",
       "       -0.03005888, -0.17473607,  0.09612537,  0.06901   , -0.07699889,\n",
       "       -0.0036542 , -0.03618549,  0.03898519,  0.12175167,  0.15467794,\n",
       "       -0.01883229, -0.11444954,  0.08837811,  0.24612263,  0.13104303,\n",
       "        0.19503498, -0.14276145, -0.03948844, -0.07201289,  0.11560602,\n",
       "        0.16577166,  0.09263071, -0.1014535 ,  0.07276203, -0.09087972],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_fe[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c79dbf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b587c03b",
   "metadata": {},
   "source": [
    "#### CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cbd01905",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Word2Vec(sentences=training_samples, min_count=1,\n",
    "                                vector_size=200, window=5,sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "aeb79e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [0.13840732, -0.22188556, -0.16674927, 0.21198...\n",
       "1      [0.024595836, -0.051688176, -0.05450714, 0.417...\n",
       "2      [0.12491854, -0.1878231, -0.13842414, 0.215766...\n",
       "3      [0.13495147, -0.13610925, -0.0126397405, 0.166...\n",
       "4      [0.10271251, -0.08723561, -0.02034141, 0.28603...\n",
       "                             ...                        \n",
       "995    [0.03471159, -0.028713746, 0.017016081, 0.2927...\n",
       "996    [0.031813428, 0.0014286998, 0.056709036, 0.282...\n",
       "997    [0.09087726, -0.12985303, -0.072528854, 0.1739...\n",
       "998    [0.07008254, -0.11099894, -0.08870851, 0.27969...\n",
       "999    [0.07954392, -0.12657845, -0.086049646, 0.2119...\n",
       "Name: text, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_fe = df['text'][:1000].apply(lambda text: np.mean([model2.wv[word] for word in text.split() if word in model2.wv],axis=0))\n",
    "samples_fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3fc7e43f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.13840732, -0.22188556, -0.16674927,  0.21198055,  0.26666427,\n",
       "       -0.20671472, -0.3379931 ,  0.6172297 , -0.00439082,  0.48558506,\n",
       "       -0.14324999, -0.24175769, -0.19305897,  0.4613716 ,  0.02427767,\n",
       "       -0.22854427, -0.30557477, -0.11403788,  0.06203   , -0.4840235 ,\n",
       "        0.423883  , -0.35360068,  0.11669571,  0.10242148,  0.09529186,\n",
       "       -0.27462274, -0.09539352,  0.03002821, -0.42187926,  0.21761674,\n",
       "        0.2616787 , -0.07417741, -0.02443583, -0.26509362,  0.28882137,\n",
       "        0.06539727,  0.34661406,  0.15762623, -0.26002583, -0.2097789 ,\n",
       "       -0.00375025, -0.16684906, -0.10815615,  0.37258038,  0.30269352,\n",
       "        0.01462196, -0.17952058,  0.02066638,  0.3475885 ,  0.2867722 ,\n",
       "        0.07128815, -0.09315752, -0.21413389, -0.21104342, -0.07772175,\n",
       "        0.11919308,  0.13500172, -0.14463179, -0.6000512 , -0.01285411,\n",
       "       -0.31966248,  0.02187579, -0.20989707,  0.22210233,  0.06149295,\n",
       "        0.10397222,  0.07188512,  0.47626677, -0.28294414,  0.24794666,\n",
       "       -0.05001966, -0.11238374,  0.4619991 , -0.16624443,  0.02301083,\n",
       "        0.07423077,  0.37713352, -0.2314502 , -0.44085327, -0.1569296 ,\n",
       "       -0.28664652, -0.24873611, -0.15670052,  0.40527987, -0.12887067,\n",
       "       -0.06501857, -0.10375468,  0.41431442, -0.04322964, -0.17909582,\n",
       "        0.16562091,  0.3031655 ,  0.25627217,  0.29500726,  0.257162  ,\n",
       "        0.29071125,  0.10482457, -0.1455973 ,  0.08772384,  0.30927724,\n",
       "       -0.23616019,  0.45623085,  0.08172964, -0.27330843, -0.37896553,\n",
       "       -0.30092487,  0.21501055,  0.23977847,  0.04498053, -0.36479688,\n",
       "       -0.2606871 , -0.15233238, -0.10755472,  0.09006124,  0.22135657,\n",
       "       -0.2081354 ,  0.29182413, -0.4291087 ,  0.02623192, -0.05680399,\n",
       "       -0.00355637,  0.16263758,  0.24445263, -0.02547186,  0.10400284,\n",
       "        0.21782789, -0.01807374, -0.16509096,  0.18604738, -0.3661395 ,\n",
       "        0.15400752,  0.10038864, -0.25319302, -0.3517296 , -0.04496713,\n",
       "       -0.00915177, -0.13365725, -0.3573529 , -0.2396405 , -0.42244816,\n",
       "        0.37906283, -0.28751   , -0.23993297, -0.31933436, -0.07048017,\n",
       "       -0.17960744, -0.16787173,  0.12971002, -0.29831532,  0.03935333,\n",
       "       -0.04638565, -0.27970177,  0.03383502,  0.17035082, -0.17286202,\n",
       "        0.23283854,  0.13681605,  0.28074586, -0.26364985,  0.13578697,\n",
       "        0.19893464,  0.12233391, -0.36954898,  0.04115124,  0.0134705 ,\n",
       "       -0.07316193,  0.09120166, -0.60478014, -0.05218568,  0.36232373,\n",
       "       -0.1473575 ,  0.1521072 ,  0.09919585, -0.11115888,  0.02890735,\n",
       "        0.08300333, -0.26341358,  0.17376032,  0.20562288,  0.06563891,\n",
       "       -0.00680064, -0.0475391 ,  0.07950129,  0.18765919,  0.25030556,\n",
       "        0.05963234, -0.1976833 ,  0.22578575,  0.49169436,  0.21990569,\n",
       "        0.27811486, -0.14143986, -0.07356326, -0.25560898,  0.24340837,\n",
       "        0.304847  ,  0.16021883, -0.21211672,  0.06000444, -0.02536615],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_fe[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bad6961",
   "metadata": {},
   "source": [
    "### fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bebe2e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "aaefc4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fasttext = FastText(sentences=training_samples, min_count=1,\n",
    "                                vector_size=200, window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "01e506dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [-0.50773275, 0.058324426, -0.016685331, -0.23...\n",
       "1      [-0.6261648, 0.22341044, -0.025309082, -0.1853...\n",
       "2      [-0.5583351, 0.13777289, -0.0191038, -0.214873...\n",
       "3      [-0.5252659, 0.17695999, -0.023132937, -0.1707...\n",
       "4      [-0.56707567, 0.22010595, -0.040985283, -0.208...\n",
       "                             ...                        \n",
       "995    [-0.5925086, 0.22992893, -0.023463774, -0.1797...\n",
       "996    [-0.6923324, 0.2846841, -0.02709923, -0.184496...\n",
       "997    [-0.5767198, 0.124370866, -0.02409606, -0.2200...\n",
       "998    [-0.60494703, 0.17404887, -0.01755056, -0.1979...\n",
       "999    [-0.60044765, 0.18412969, -0.020632178, -0.195...\n",
       "Name: text, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_fe = df['text'][:1000].apply(lambda text: np.mean([model_fasttext.wv[word] for word in text.split() if word in model_fasttext.wv],axis=0))\n",
    "samples_fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0b27f842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.50773275,  0.05832443, -0.01668533, -0.23983948,  0.16388766,\n",
       "       -0.10223313, -0.06961938, -0.25564766, -0.37241256, -0.2716643 ,\n",
       "       -0.26845917,  0.1034795 , -0.56889915, -0.23011273, -0.20482369,\n",
       "       -0.24155428,  0.16923608, -0.27766922,  0.2990739 ,  0.2587533 ,\n",
       "       -0.02013325,  0.12738556, -0.11223251, -0.07805966, -0.26823196,\n",
       "        0.36440673, -0.22199133,  0.09656067,  0.3537114 , -0.01740418,\n",
       "        0.07951099, -0.04699205,  0.19276625,  0.06142053,  0.48876423,\n",
       "        0.61698717, -0.03414697, -0.10478519, -0.0306671 ,  0.17387828,\n",
       "        0.08842253,  0.27012444, -0.08327803,  0.4866028 , -0.03499604,\n",
       "       -0.12785476,  0.13906701,  0.0974864 ,  0.1357064 , -0.54331976,\n",
       "       -0.90102893,  0.12208855,  0.14443061, -0.0648033 ,  0.15065034,\n",
       "        0.12381642, -0.24882084, -0.13076548,  0.08907124, -0.05459258,\n",
       "       -0.35284856,  0.26514274, -0.2092277 , -0.07168856, -0.10770483,\n",
       "        0.38447186,  0.3445882 ,  0.03584883, -0.11163683, -0.50216395,\n",
       "        0.17361967,  0.3333396 ,  0.13858385,  0.13824101,  0.5148742 ,\n",
       "       -0.8568277 ,  0.01680446,  0.19933219,  0.11619666, -0.23144357,\n",
       "       -0.3364432 , -0.3713914 , -0.1738177 , -0.5877947 ,  0.02860881,\n",
       "        0.06607936,  0.51530856,  0.5456771 ,  0.22646563,  0.2293539 ,\n",
       "       -0.07969148,  0.5397558 ,  0.2599469 , -0.32344943,  0.06298643,\n",
       "        0.08685633,  0.16074742, -0.4099736 , -0.17512047, -0.00843375,\n",
       "        0.15941893, -0.41189054, -0.37908593, -0.09452981, -0.23855536,\n",
       "       -0.4554595 ,  0.05816483, -0.28872842,  0.17172961, -0.29021746,\n",
       "       -0.48195255, -0.00428212,  0.10259493, -0.42890728,  0.6031536 ,\n",
       "        0.00768352,  0.23221704, -0.1885369 , -0.26642933, -0.21435791,\n",
       "       -0.63475525, -0.28394765,  0.30081588,  0.00915313,  0.31097153,\n",
       "        0.15706654,  0.20226358,  0.15310326, -0.38252726,  0.30982873,\n",
       "       -0.1412162 , -0.21375324,  0.11421863,  0.24055228, -0.15573643,\n",
       "       -0.14388748,  0.18834852,  0.01784863,  0.25006536, -0.066117  ,\n",
       "        0.17096116,  0.18268469, -0.29022866,  0.11832461,  0.05696504,\n",
       "        0.16460535,  0.28051972,  0.3896146 , -0.08429947, -0.04466953,\n",
       "        0.19039431, -0.20607212, -0.50811636, -0.21972285,  0.3854566 ,\n",
       "       -0.12613042, -0.2613583 , -0.2364787 ,  0.0029173 , -0.29622337,\n",
       "       -0.2765652 , -0.04295895, -0.13429993, -0.00786226, -0.10700382,\n",
       "        0.5446351 ,  0.17876127, -0.05012495, -0.42065027, -0.16349916,\n",
       "        0.17664577,  0.22402664,  0.00184794, -0.0304616 ,  0.17830808,\n",
       "        0.00742435, -0.06841308,  0.53137845, -0.03761034, -0.4087087 ,\n",
       "        0.08899715, -0.10983808, -0.21118799, -0.04496803, -0.1018341 ,\n",
       "        0.1621545 , -0.39122498,  0.14129852, -0.18337095, -0.2474335 ,\n",
       "        0.04805376, -0.1335624 ,  0.25703502, -0.21790354, -0.06770694,\n",
       "        0.01781383, -0.2827093 , -0.1074682 ,  0.3552955 ,  0.58036244],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_fe[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06a9223",
   "metadata": {},
   "source": [
    "### GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "025c8915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7af36697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=-------------------------------------------------] 3.1% 23.4/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[======--------------------------------------------] 12.8% 97.1/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===========---------------------------------------] 22.7% 172.2/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[================----------------------------------] 32.3% 245.0/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=====================-----------------------------] 42.2% 320.2/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==========================------------------------] 52.5% 398.5/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===============================-------------------] 63.0% 477.5/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[====================================--------------] 73.3% 556.2/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=========================================---------] 83.6% 634.4/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==============================================----] 93.8% 711.7/758.5MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "glove_vectors = gensim.downloader.load('glove-twitter-200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ef59c14d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [0.08796369, 0.10185423, 0.07532824, 0.1922056...\n",
       "1      [0.13417737, 0.14988533, 0.006529619, 0.125828...\n",
       "2      [0.093451075, 0.14090881, 0.041187413, 0.13554...\n",
       "3      [0.10934508, 0.14650647, 0.08094985, 0.1336431...\n",
       "4      [0.14064875, 0.082987, 0.08059668, 0.14689596,...\n",
       "                             ...                        \n",
       "995    [0.13349752, 0.052978683, 0.09856825, 0.120453...\n",
       "996    [0.06676295, 0.0028329766, 0.086873844, 0.1708...\n",
       "997    [0.0820499, 0.041169856, 0.12953268, 0.1430508...\n",
       "998    [0.13366817, 0.121703036, 0.1199855, 0.1741929...\n",
       "999    [0.09286539, 0.074824266, 0.091667704, 0.19394...\n",
       "Name: text, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_fe = df['text'][:1000].apply(lambda text: np.mean([glove_vectors[word] for word in text.split() if word in glove_vectors],axis=0))\n",
    "samples_fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "afb5343c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.79636928e-02,  1.01854227e-01,  7.53282383e-02,  1.92205608e-01,\n",
       "       -1.37415705e-02,  8.48635882e-02,  3.76222759e-01, -1.37983849e-02,\n",
       "       -1.07305691e-01, -2.10104257e-01,  3.73063423e-02, -2.93353237e-02,\n",
       "       -9.10961866e-01, -4.80163097e-02,  1.43521931e-02, -2.03544050e-02,\n",
       "        3.37492824e-02,  4.11222316e-03, -6.41812757e-02, -1.36727532e-02,\n",
       "        1.04375824e-01,  9.28258151e-02, -4.31509241e-02, -1.82961021e-02,\n",
       "       -3.67635973e-02,  8.76572609e-01,  1.41530558e-01,  5.91314435e-02,\n",
       "        7.09164590e-02,  2.61355881e-02,  5.92161790e-02,  7.34258397e-03,\n",
       "       -9.62192640e-02, -8.02505091e-02, -2.61530410e-02,  2.86175273e-02,\n",
       "        4.45067883e-02, -7.76280742e-03,  1.52504668e-01, -1.05464548e-01,\n",
       "        3.70857447e-01, -9.06854402e-04,  1.07127540e-01, -1.25762112e-02,\n",
       "        6.68023601e-02, -7.42668211e-02,  1.56346038e-01, -6.78395554e-02,\n",
       "       -1.68255791e-02, -6.32421896e-02, -9.18123871e-02,  1.93099473e-02,\n",
       "       -6.35614693e-02, -3.37417461e-02, -1.44129351e-01,  8.97283852e-02,\n",
       "       -1.37949884e-01,  8.41208398e-02, -4.92541268e-02,  7.17232656e-03,\n",
       "       -4.53989813e-03,  1.08949937e-01, -9.59320664e-02, -4.90799658e-02,\n",
       "        1.02432124e-01,  1.19824216e-01, -6.36754856e-02,  1.84223838e-02,\n",
       "       -7.59909078e-02, -2.02053580e-02, -6.37047961e-02, -9.49360356e-02,\n",
       "       -4.11863364e-02, -1.01505138e-01,  3.25559944e-01,  5.57916835e-02,\n",
       "        1.41386455e-02, -1.49367407e-01, -7.45657906e-02,  5.25573781e-03,\n",
       "        4.93502140e-01,  1.35524467e-01,  3.46826687e-02,  1.49990320e-01,\n",
       "        1.20871402e-02,  3.57456282e-02,  1.14405639e-01, -9.98453647e-02,\n",
       "       -2.23087400e-01, -2.31280234e-02,  9.80873257e-02,  1.11313716e-01,\n",
       "        3.91218960e-02,  7.54874956e-04,  6.77397028e-02,  1.72155481e-02,\n",
       "        6.87255114e-02, -1.30761236e-01,  9.47020352e-02,  5.29660992e-02,\n",
       "        1.12815097e-01,  1.55381141e-02,  7.42217107e-03,  3.23190466e-02,\n",
       "        1.56237304e-01, -1.33596569e-01,  1.41536787e-01,  3.24703678e-02,\n",
       "       -1.47674024e-01, -7.03007132e-02,  2.16020551e-02,  7.15287402e-02,\n",
       "        3.39542292e-02, -8.50447118e-02, -1.22156717e-01,  2.12621293e-03,\n",
       "        2.94802375e-02,  9.65014026e-02,  3.16442661e-02,  2.28054583e-01,\n",
       "        5.48239611e-02,  1.39910271e-02,  1.24417022e-01, -1.05989546e-01,\n",
       "        7.74071505e-03,  6.26172200e-02,  9.66835171e-02,  2.84171663e-03,\n",
       "        4.07767683e-01,  2.61601084e-03, -2.73976773e-02,  1.09322108e-01,\n",
       "        2.53244024e-02,  1.11526297e-02, -9.94638279e-02, -3.82476486e-02,\n",
       "        9.87846106e-02,  6.79738969e-02, -4.59590405e-02, -1.34505689e-01,\n",
       "        1.56029746e-01,  7.87467808e-02, -1.29045069e-01,  1.79049209e-01,\n",
       "        5.60345799e-02, -2.99671534e-02, -6.64750263e-02,  2.13026050e-02,\n",
       "       -7.95498118e-02,  7.01043382e-02,  9.13564265e-02,  1.69487193e-01,\n",
       "       -4.64750290e+00,  6.28999323e-02, -2.29340848e-02,  3.60020995e-02,\n",
       "        1.86425149e-01,  2.20947079e-02, -5.72207384e-02,  9.20530632e-02,\n",
       "        2.40400340e-02,  1.06509648e-01, -3.90295163e-02, -6.88527972e-02,\n",
       "        1.17612459e-01, -7.12309405e-02, -6.88477457e-02, -1.78142712e-02,\n",
       "        1.32089511e-01, -1.00424692e-01,  5.08425981e-02,  4.24704477e-02,\n",
       "        2.40698587e-02,  1.31437644e-01, -2.62530837e-02, -2.88341530e-02,\n",
       "        3.83044034e-02, -4.72917780e-02, -1.75871290e-02,  7.76106864e-02,\n",
       "       -8.37558880e-03,  3.98935564e-02, -1.03536054e-01, -6.41299710e-02,\n",
       "       -4.75002499e-03,  8.14821273e-02,  5.76693490e-02, -8.52544531e-02,\n",
       "       -1.13702580e-01,  2.27453485e-02, -1.31648276e-02, -5.85670806e-02,\n",
       "        9.05422568e-02,  7.31913596e-02, -4.09354130e-03, -1.06979169e-01,\n",
       "        1.68659806e-01,  1.74822763e-01, -2.92058680e-02, -1.46849960e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_fe[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346fc5eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
