{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5db0976e-422a-4861-be36-0d77394a2346",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/alexacole/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alexacole/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/alexacole/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "import sklearn\n",
    "#import spacy\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75b5a047-4688-47da-af77-705569e87c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['source', 'id', 'text'],\n",
       "        num_rows: 1392522\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset('artem9k/ai-text-detection-pile')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a85aadf-da6e-49da-a4a7-9987e49b454c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>human</td>\n",
       "      <td>0</td>\n",
       "      <td>12 Years a Slave: An Analysis of the Film Essa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>human</td>\n",
       "      <td>1</td>\n",
       "      <td>20+ Social Media Post Ideas to Radically Simpl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>human</td>\n",
       "      <td>2</td>\n",
       "      <td>2022 Russian Invasion of Ukraine in Global Med...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>human</td>\n",
       "      <td>3</td>\n",
       "      <td>533 U.S. 27 (2001) Kyllo v. United States: The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>human</td>\n",
       "      <td>4</td>\n",
       "      <td>A Charles Schwab Corporation Case Essay\\n\\nCha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source  id                                               text\n",
       "0  human   0  12 Years a Slave: An Analysis of the Film Essa...\n",
       "1  human   1  20+ Social Media Post Ideas to Radically Simpl...\n",
       "2  human   2  2022 Russian Invasion of Ukraine in Global Med...\n",
       "3  human   3  533 U.S. 27 (2001) Kyllo v. United States: The...\n",
       "4  human   4  A Charles Schwab Corporation Case Essay\\n\\nCha..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(dataset['train'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba0f467",
   "metadata": {},
   "source": [
    "## Reformat Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bc4493a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['human', 'ai'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['source'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82fa5be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['source'] = [1 if x == 'ai' else 0 for x in df['source']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bbe1c22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12 Years a Slave: An Analysis of the Film Essa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20+ Social Media Post Ideas to Radically Simpl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2022 Russian Invasion of Ukraine in Global Med...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>533 U.S. 27 (2001) Kyllo v. United States: The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>A Charles Schwab Corporation Case Essay\\n\\nCha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source  id                                               text\n",
       "0       0   0  12 Years a Slave: An Analysis of the Film Essa...\n",
       "1       0   1  20+ Social Media Post Ideas to Radically Simpl...\n",
       "2       0   2  2022 Russian Invasion of Ukraine in Global Med...\n",
       "3       0   3  533 U.S. 27 (2001) Kyllo v. United States: The...\n",
       "4       0   4  A Charles Schwab Corporation Case Essay\\n\\nCha..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc858b14-9dbe-470f-928f-90881e776f13",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32b518de-439b-4bcc-93e2-534898f17a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for preprocessing\n",
    "def remove_urls(text):\n",
    "    return re.sub(r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\", \" \", text) # regex taken from https://www.geeksforgeeks.org/python-check-url-string/\n",
    "\n",
    "def remove_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def remove_extra_whitespace(text):\n",
    "    text = text.strip()\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def lemmatizer(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    l = nltk.stem.WordNetLemmatizer()\n",
    "    tokens = [l.lemmatize(token) for token in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# eliminate punctuation\n",
    "def remove_punctuation(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def tokenize_pre_process(text): # for preprocessing using this link: https://spotintelligence.com/2022/12/21/nltk-preprocessing-pipeline/\n",
    "    # tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # remove stop words\n",
    "    stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "\n",
    "    # remove top 10% most frequent words \n",
    "    fdist = nltk.FreqDist(tokens)\n",
    "    tokens = [token for token in tokens if fdist[token] < fdist.N() * 0.1]\n",
    "\n",
    "    # stemming\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    # eliminate punctuation\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8857fa40-58c8-4c7b-a125-075104c50009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # encoding to ascii\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    \n",
    "    # convert text to lower case\n",
    "    text = text.lower()\n",
    "\n",
    "    # remove html tags \n",
    "    text = remove_html(text)\n",
    "\n",
    "    # remove urls \n",
    "    text = remove_urls(text)\n",
    "\n",
    "    # remove extra whitespace\n",
    "    text = remove_extra_whitespace(text)\n",
    "\n",
    "    # remove stop words\n",
    "    text = remove_stop_words(text)\n",
    "    \n",
    "    # remove punctuation\n",
    "    text = remove_punctuation(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1afd640-75a0-4bae-95ce-bc0a5a4b8360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text2(text):\n",
    "    # encoding to ascii\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    \n",
    "    # convert text to lower case\n",
    "    text = text.lower()\n",
    "\n",
    "    # remove html tags \n",
    "    text = remove_html(text)\n",
    "\n",
    "    # remove urls \n",
    "    text = remove_urls(text)\n",
    "\n",
    "    # remove extra whitespace\n",
    "    text = remove_extra_whitespace(text)\n",
    "\n",
    "    # remove stop words\n",
    "    text = remove_stop_words(text)\n",
    "    \n",
    "    # remove punctuation\n",
    "    text = remove_punctuation(text)\n",
    "\n",
    "    # lemmatize words\n",
    "    text = lemmatizer(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343436b3-c476-4b04-babb-29588e7cb236",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f481c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "import gensim.downloader\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7611b432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate document embeddings\n",
    "# Extracted on Google\n",
    "def get_doc_embedding(doc, model):\n",
    "    vectors = [model.wv[word.lower()] for word in doc.split() if word in model.wv]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3dff8548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_embedding_glove(doc, model):\n",
    "    vectors = [model[word.lower()] for word in doc.split() if word in model]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c8498098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for features\n",
    "\n",
    "def count_vectorizer(data_train, data_test):\n",
    "    vec = CountVectorizer(preprocessor=preprocess_text2,max_df=0.9,min_df=0.1)\n",
    "    train = vec.fit_transform(data_train)\n",
    "    test = vec.transform(data_test)\n",
    "    return train, test\n",
    "\n",
    "def tfidf(data_train, data_test):\n",
    "    tfidf = TfidfVectorizer(preprocessor=preprocess_text2,max_df=0.9,min_df=0.1)\n",
    "    train = tfidf.fit_transform(data_train)\n",
    "    test = tfidf.transform(data_test)\n",
    "    return train, test \n",
    "\n",
    "def word2vec_skipgram(data_train, data_test):\n",
    "    # preprocess and tokenize samples\n",
    "    train_samples = data_train.text.apply(preprocess_text2)\n",
    "    train_samples2 = train_samples.apply(word_tokenize)\n",
    "    test_samples = data_test.text.apply(preprocess_text2)\n",
    "    \n",
    "    # skipgram word2vec model\n",
    "    model1 = Word2Vec(sentences=train_samples2, min_count=1,vector_size=200, window=5,sg = 1)\n",
    "    \n",
    "    # get averaged word vectors for each sample\n",
    "    train = np.array([get_doc_embedding(texts, model1) for texts in train_samples])\n",
    "    test = np.array([get_doc_embedding(texts, model1) for texts in test_samples])\n",
    "    \n",
    "    return train, test \n",
    "\n",
    "def word2vec_cbow(data_train, data_test):\n",
    "    # preprocess and tokenize samples\n",
    "    train_samples = data_train.text.apply(preprocess_text2)\n",
    "    train_samples2 = train_samples.apply(word_tokenize)\n",
    "    test_samples = data_test.text.apply(preprocess_text2)\n",
    "    \n",
    "    # cbow word2vec model\n",
    "    model1 = Word2Vec(sentences=train_samples2, min_count=1,vector_size=200, window=5,sg=0)\n",
    "    \n",
    "    # get averaged word vectors for each sample\n",
    "    train = np.array([get_doc_embedding(texts, model1) for texts in train_samples])\n",
    "    test = np.array([get_doc_embedding(texts, model1) for texts in test_samples])\n",
    "    \n",
    "    return train, test \n",
    "\n",
    "def fast_text(data_train, data_test):\n",
    "    # preprocess and tokenize samples\n",
    "    train_samples = data_train.text.apply(preprocess_text2)\n",
    "    train_samples2 = train_samples.apply(word_tokenize)\n",
    "    test_samples = data_test.text.apply(preprocess_text2)\n",
    "    \n",
    "    # FastText Model\n",
    "    model_fasttext = FastText(sentences=train_samples2, min_count=1,vector_size=200, window=5)\n",
    "    \n",
    "    # get averaged word vectors for each sample\n",
    "    train = np.array([get_doc_embedding(texts, model_fasttext) for texts in train_samples])\n",
    "    test = np.array([get_doc_embedding(texts, model_fasttext) for texts in test_samples])\n",
    "    \n",
    "    return train, test \n",
    "    \n",
    "def glove_twitter_200(data_train, data_test):\n",
    "    # preprocess samples\n",
    "    train_samples = data_train.text.apply(preprocess_text2)\n",
    "    test_samples = data_test.text.apply(preprocess_text2)\n",
    "    \n",
    "    # pretrained glove vectors from twitter-200\n",
    "    glove_vectors = gensim.downloader.load('glove-twitter-200')\n",
    "    \n",
    "    # get averaged word vectors for each sample\n",
    "    train = np.array([get_doc_embedding_glove(texts, glove_vectors) for texts in train_samples])\n",
    "    test = np.array([get_doc_embedding_glove(texts, glove_vectors) for texts in test_samples])\n",
    "    \n",
    "    return train, test \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1628e8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc2vec(data, data_test):\n",
    "    '''\n",
    "    https://www.geeksforgeeks.org/doc2vec-in-nlp/\n",
    "    '''\n",
    "    \n",
    "    tagged_data = [TaggedDocument(words=word_tokenize(doc.lower()),\n",
    "                              tags=[str(i)]) for i,doc in enumerate(data)]\n",
    "    # train the Doc2vec model\n",
    "    model = Doc2Vec(vector_size=20,\n",
    "                    min_count=2, epochs=50)\n",
    "    model.build_vocab(tagged_data)\n",
    "    model.train(tagged_data,\n",
    "                total_examples=model.corpus_count,\n",
    "                epochs=model.epochs)\n",
    "     \n",
    "    # get the document vectors\n",
    "    document_vectors = [model.infer_vector(\n",
    "        word_tokenize(doc.lower())) for doc in data]\n",
    "    document_vectors_test = [model.infer_vector(\n",
    "        word_tokenize(doc.lower())) for doc in data_test]\n",
    "\n",
    "    return document_vectors, document_vectors_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac66e8d",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "346fc5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import scipy.stats as stats\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbb18416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CI(metric, confidence):\n",
    "    a,b = stats.t.interval(confidence, \n",
    "                         len(metric)-1, \n",
    "                         loc=metric.mean(), \n",
    "                         scale=metric.std(ddof=1)/np.sqrt(len(metric)))\n",
    "    return a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81a6de85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(X_train, X_test, t_train, t_test, model, model_name, confidence=0.95, scoring='accuracy'):\n",
    "    y_train = model.predict(X_train)\n",
    "    y_test = model.predict(X_test)\n",
    "    \n",
    "    scores = cross_val_score(model,\n",
    "                             X_train, \n",
    "                             t_train, \n",
    "                             scoring=scoring, \n",
    "                             cv=KFold(10, shuffle=True, random_state=0))\n",
    "    \n",
    "    a,b = CI(scores, confidence)\n",
    "    \n",
    "    print(f'==================={model_name} Performance=====================')\n",
    "    print('95% CI = [', a, b, ']')\n",
    "    print('Train: ', classification_report(t_train, y_train))\n",
    "    print('Test: ', classification_report(t_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25800138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_test_with_selected_features(df, selected_features):\n",
    "    # Vectorize the documents (using non-preprocessed data)\n",
    "    v = selected_features(df.text)\n",
    "\n",
    "    # Set up the data and labels\n",
    "    X = np.array(v)\n",
    "    t = data.source\n",
    "    X_train, X_test, t_train, t_test = train_test_split(X, t, test_size=0.2, random_state=0)\n",
    "    return X_train, X_test, t_train, t_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba594eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent ai: 35.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q7/lxfgdwq942qdsd3_mlwlm7p00000gn/T/ipykernel_40154/2078218284.py:6: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  data = df.groupby('source', group_keys=False).apply(lambda x: x.sample(frac=0.001, random_state=0))\n"
     ]
    }
   ],
   "source": [
    "# Reporting the proportion of samples that are ai generated\n",
    "print('Percent ai:', round(df[df['source'] == 1].shape[0]/df[df['source'] == 0].shape[0]*100, 3))\n",
    "\n",
    "# Taking a stratified sample of 0.1% of the data\n",
    "# maintaining same proportions of human and ai samples\n",
    "data = df.groupby('source', group_keys=False).apply(lambda x: x.sample(frac=0.001, random_state=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8045b49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>198049</td>\n",
       "      <td>Overview\\n\\nBatman and Psychology: A Dark and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>12919</td>\n",
       "      <td>The Use of Psychedelic Drugs in Treating Depre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>979845</td>\n",
       "      <td>member the day like it was yesterday. \\n My mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>73499</td>\n",
       "      <td>Legislative Branch Power, Its Limits and Expan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>44380</td>\n",
       "      <td>Growth and Fall of Vader Corporation Report\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>1</td>\n",
       "      <td>1052501</td>\n",
       "      <td>\"Learn Python the Hard Way\" by Zed Shaw\\n\\t\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>1</td>\n",
       "      <td>1277472</td>\n",
       "      <td>Cape Town - The South African Humanist Associa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>1</td>\n",
       "      <td>1086015</td>\n",
       "      <td>We are only days away from the 2016 NFL Draft,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>1</td>\n",
       "      <td>1338694</td>\n",
       "      <td>The number of people infected with Zika virus ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>1</td>\n",
       "      <td>1189675</td>\n",
       "      <td>Dorothy Lee Hough - May 24, 2017\\n\\nDorothy Le...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      source       id                                               text\n",
       "0          0   198049  Overview\\n\\nBatman and Psychology: A Dark and ...\n",
       "1          0    12919  The Use of Psychedelic Drugs in Treating Depre...\n",
       "2          0   979845  member the day like it was yesterday. \\n My mo...\n",
       "3          0    73499  Legislative Branch Power, Its Limits and Expan...\n",
       "4          0    44380  Growth and Fall of Vader Corporation Report\\n\\...\n",
       "...      ...      ...                                                ...\n",
       "3995       1  1052501   \"Learn Python the Hard Way\" by Zed Shaw\\n\\t\\t...\n",
       "3996       1  1277472  Cape Town - The South African Humanist Associa...\n",
       "3997       1  1086015  We are only days away from the 2016 NFL Draft,...\n",
       "3998       1  1338694  The number of people infected with Zika virus ...\n",
       "3999       1  1189675  Dorothy Lee Hough - May 24, 2017\\n\\nDorothy Le...\n",
       "\n",
       "[4000 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human = df[df.source == 0]\n",
    "human = human.sample(2000, random_state=0)\n",
    "ai = df[df.source == 1].sample(2000, random_state=0)\n",
    "equal = pd.concat([human,ai], ignore_index=True)\n",
    "equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "997e4bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(data):\n",
    "    tex = []\n",
    "    for i in range(len(data.text)):\n",
    "        tex += [preprocess_text2(data.text.iloc[i])]\n",
    "    prepped = data.insert(len(data.columns),'prep_text', tex)\n",
    "    return prepped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f509ae5",
   "metadata": {},
   "source": [
    "### Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26ad470d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af7cec5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, t_train, t_test = train_test_split(np.array(equal.text), np.array(equal.source), test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "859fb5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_pipe = Pipeline([('mnb', MultinomialNB())])\n",
    "mnb_param = {'mnb__alpha': [0.2, 0.4, 0.6, 0.8, 1], 'mnb__force_alpha': [True, False]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7aa48ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q7/lxfgdwq942qdsd3_mlwlm7p00000gn/T/ipykernel_40154/3804508306.py:6: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "===================MNB with Unigrams Performance=====================\n",
      "95% CI = [ 0.6976949069861126 0.7373050930138875 ]\n",
      "Train:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.71      0.73      1593\n",
      "           1       0.72      0.75      0.74      1607\n",
      "\n",
      "    accuracy                           0.73      3200\n",
      "   macro avg       0.73      0.73      0.73      3200\n",
      "weighted avg       0.73      0.73      0.73      3200\n",
      "\n",
      "Test:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.69      0.71       407\n",
      "           1       0.69      0.73      0.71       393\n",
      "\n",
      "    accuracy                           0.71       800\n",
      "   macro avg       0.71      0.71      0.71       800\n",
      "weighted avg       0.71      0.71      0.71       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply Count Vectorizer\n",
    "X_train_cv, X_test_cv = count_vectorizer(X_train, X_test)\n",
    "gs3 = GridSearchCV(mnb_pipe, \n",
    "                   param_grid=mnb_param,\n",
    "                   cv=KFold(n_splits=5, shuffle=True, random_state=0), \n",
    "                   scoring='accuracy',\n",
    "                   verbose=1, \n",
    "                   n_jobs=-1, \n",
    "                   refit=True)\n",
    "\n",
    "gs3.fit(X_train_cv, t_train)\n",
    "mnb = gs3.best_estimator_\n",
    "evaluation(X_train_cv, X_test_cv, t_train, t_test, mnb, 'MNB with Unigrams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6b8da30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q7/lxfgdwq942qdsd3_mlwlm7p00000gn/T/ipykernel_40154/3804508306.py:6: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "===================MNB with TF-IDF Performance=====================\n",
      "95% CI = [ 0.7239073216747745 0.7460926783252257 ]\n",
      "Train:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.68      0.73      1593\n",
      "           1       0.72      0.81      0.76      1607\n",
      "\n",
      "    accuracy                           0.74      3200\n",
      "   macro avg       0.75      0.74      0.74      3200\n",
      "weighted avg       0.75      0.74      0.74      3200\n",
      "\n",
      "Test:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.69      0.72       407\n",
      "           1       0.71      0.78      0.74       393\n",
      "\n",
      "    accuracy                           0.73       800\n",
      "   macro avg       0.73      0.73      0.73       800\n",
      "weighted avg       0.74      0.73      0.73       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply TFIDF\n",
    "X_train_tfidf, X_test_tfidf = tfidf(X_train, X_test)\n",
    "\n",
    "gs3 = GridSearchCV(mnb_pipe, \n",
    "                   param_grid=mnb_param,\n",
    "                   cv=KFold(n_splits=5, shuffle=True, random_state=0), \n",
    "                   scoring='accuracy',\n",
    "                   verbose=1, \n",
    "                   n_jobs=-1, \n",
    "                   refit=True)\n",
    "\n",
    "gs3.fit(X_train_tfidf, t_train)\n",
    "mnb = gs3.best_estimator_\n",
    "evaluation(X_train_tfidf, X_test_tfidf, t_train, t_test, mnb, 'MNB with TF-IDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0eac8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_pipe2 = Pipeline([('scaler', MinMaxScaler()),('mnb', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2f0e4ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q7/lxfgdwq942qdsd3_mlwlm7p00000gn/T/ipykernel_40154/3804508306.py:6: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "===================MNB with word2vec skip-gram Performance=====================\n",
      "95% CI = [ 0.7042118449338021 0.7257881550661979 ]\n",
      "Train:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.71      0.72      1593\n",
      "           1       0.72      0.72      0.72      1607\n",
      "\n",
      "    accuracy                           0.72      3200\n",
      "   macro avg       0.72      0.72      0.72      3200\n",
      "weighted avg       0.72      0.72      0.72      3200\n",
      "\n",
      "Test:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.70      0.72       407\n",
      "           1       0.71      0.75      0.73       393\n",
      "\n",
      "    accuracy                           0.72       800\n",
      "   macro avg       0.73      0.73      0.72       800\n",
      "weighted avg       0.73      0.72      0.72       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_train = pd.DataFrame(X_train, columns=['text']) \n",
    "data_test = pd.DataFrame(X_test, columns=['text'])\n",
    "\n",
    "# Apply word2vec skipgram\n",
    "X_train_skipgram, X_test_skipgram = word2vec_skipgram(data_train, data_test)\n",
    "\n",
    "gs3 = GridSearchCV(mnb_pipe2, \n",
    "                   param_grid=mnb_param,\n",
    "                   cv=KFold(n_splits=5, shuffle=True, random_state=0), \n",
    "                   scoring='accuracy',\n",
    "                   verbose=1, \n",
    "                   n_jobs=-1, \n",
    "                   refit=True)\n",
    "\n",
    "gs3.fit(X_train_skipgram, t_train)\n",
    "mnb = gs3.best_estimator_\n",
    "evaluation(X_train_skipgram, X_test_skipgram, t_train, t_test, mnb, 'MNB with word2vec skip-gram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "03d5599a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q7/lxfgdwq942qdsd3_mlwlm7p00000gn/T/ipykernel_40154/3804508306.py:6: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "===================MNB with word2vec CBOW Performance=====================\n",
      "95% CI = [ 0.6821208962125406 0.7022541037874596 ]\n",
      "Train:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.73      0.70      1593\n",
      "           1       0.71      0.65      0.68      1607\n",
      "\n",
      "    accuracy                           0.69      3200\n",
      "   macro avg       0.70      0.69      0.69      3200\n",
      "weighted avg       0.70      0.69      0.69      3200\n",
      "\n",
      "Test:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.72      0.72       407\n",
      "           1       0.71      0.69      0.70       393\n",
      "\n",
      "    accuracy                           0.71       800\n",
      "   macro avg       0.71      0.71      0.71       800\n",
      "weighted avg       0.71      0.71      0.71       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply word2vec cbow\n",
    "X_train_cbow, X_test_cbow = word2vec_cbow(data_train, data_test)\n",
    "\n",
    "gs3 = GridSearchCV(mnb_pipe2, \n",
    "                   param_grid=mnb_param,\n",
    "                   cv=KFold(n_splits=5, shuffle=True, random_state=0), \n",
    "                   scoring='accuracy',\n",
    "                   verbose=1, \n",
    "                   n_jobs=-1, \n",
    "                   refit=True)\n",
    "\n",
    "gs3.fit(X_train_cbow, t_train)\n",
    "mnb = gs3.best_estimator_\n",
    "evaluation(X_train_cbow, X_test_cbow, t_train, t_test, mnb, 'MNB with word2vec CBOW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f5e0f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q7/lxfgdwq942qdsd3_mlwlm7p00000gn/T/ipykernel_40154/3804508306.py:6: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "===================MNB with Fast Text Performance=====================\n",
      "95% CI = [ 0.6526113959810843 0.6811386040189155 ]\n",
      "Train:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.67      0.67      1593\n",
      "           1       0.67      0.66      0.67      1607\n",
      "\n",
      "    accuracy                           0.67      3200\n",
      "   macro avg       0.67      0.67      0.67      3200\n",
      "weighted avg       0.67      0.67      0.67      3200\n",
      "\n",
      "Test:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.70      0.69       407\n",
      "           1       0.68      0.67      0.67       393\n",
      "\n",
      "    accuracy                           0.68       800\n",
      "   macro avg       0.68      0.68      0.68       800\n",
      "weighted avg       0.68      0.68      0.68       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply fast text\n",
    "X_train_fast, X_test_fast = fast_text(data_train, data_test)\n",
    "\n",
    "gs3 = GridSearchCV(mnb_pipe2, \n",
    "                   param_grid=mnb_param,\n",
    "                   cv=KFold(n_splits=5, shuffle=True, random_state=0), \n",
    "                   scoring='accuracy',\n",
    "                   verbose=1, \n",
    "                   n_jobs=-1, \n",
    "                   refit=True)\n",
    "\n",
    "gs3.fit(X_train_fast, t_train)\n",
    "mnb = gs3.best_estimator_\n",
    "evaluation(X_train_fast, X_test_fast, t_train, t_test, mnb, 'MNB with Fast Text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ed229a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q7/lxfgdwq942qdsd3_mlwlm7p00000gn/T/ipykernel_40154/3804508306.py:6: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "===================MNB with Glove Twitter 200 Performance=====================\n",
      "95% CI = [ 0.6955902531535414 0.7112847468464585 ]\n",
      "Train:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.69      0.70      1593\n",
      "           1       0.70      0.72      0.71      1607\n",
      "\n",
      "    accuracy                           0.71      3200\n",
      "   macro avg       0.71      0.71      0.71      3200\n",
      "weighted avg       0.71      0.71      0.71      3200\n",
      "\n",
      "Test:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.70      0.71       407\n",
      "           1       0.69      0.72      0.70       393\n",
      "\n",
      "    accuracy                           0.70       800\n",
      "   macro avg       0.71      0.71      0.70       800\n",
      "weighted avg       0.71      0.70      0.71       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply glove\n",
    "X_train_glove, X_test_glove = glove_twitter_200(data_train, data_test)\n",
    "\n",
    "gs3 = GridSearchCV(mnb_pipe2, \n",
    "                   param_grid=mnb_param,\n",
    "                   cv=KFold(n_splits=5, shuffle=True, random_state=0), \n",
    "                   scoring='accuracy',\n",
    "                   verbose=1, \n",
    "                   n_jobs=-1, \n",
    "                   refit=True)\n",
    "\n",
    "gs3.fit(X_train_glove, t_train)\n",
    "mnb = gs3.best_estimator_\n",
    "evaluation(X_train_glove, X_test_glove, t_train, t_test, mnb, 'MNB with Glove Twitter 200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0eca00d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.DataFrame(X_train, columns=['text']) \n",
    "data_test = pd.DataFrame(X_test, columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eed5cad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q7/lxfgdwq942qdsd3_mlwlm7p00000gn/T/ipykernel_40154/3804508306.py:6: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n",
      "/var/folders/q7/lxfgdwq942qdsd3_mlwlm7p00000gn/T/ipykernel_40154/3804508306.py:6: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "===================MNB with doc2vec Performance=====================\n",
      "95% CI = [ 0.673384419906898 0.754115580093102 ]\n",
      "Train:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.71      0.75      1593\n",
      "           1       0.74      0.82      0.78      1607\n",
      "\n",
      "    accuracy                           0.76      3200\n",
      "   macro avg       0.77      0.76      0.76      3200\n",
      "weighted avg       0.77      0.76      0.76      3200\n",
      "\n",
      "Test:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.73      0.75       407\n",
      "           1       0.74      0.78      0.76       393\n",
      "\n",
      "    accuracy                           0.76       800\n",
      "   macro avg       0.76      0.76      0.76       800\n",
      "weighted avg       0.76      0.76      0.76       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Doc2Vec -- Preprocessed\n",
    "prep_data(data_train)\n",
    "prep_data(data_test)\n",
    "\n",
    "X_train_Doc2Vec, X_test_Doc2Vec = doc2vec(data_train.prep_text, data_test.prep_text)\n",
    "\n",
    "gs3 = GridSearchCV(mnb_pipe2, \n",
    "                   param_grid=mnb_param,\n",
    "                   cv=KFold(n_splits=5, shuffle=True, random_state=0), \n",
    "                   scoring='accuracy',\n",
    "                   verbose=1, \n",
    "                   n_jobs=-1, \n",
    "                   refit=True)\n",
    "\n",
    "gs3.fit(X_train_Doc2Vec, t_train)\n",
    "mnb = gs3.best_estimator_\n",
    "evaluation(X_train_Doc2Vec, X_test_Doc2Vec, t_train, t_test, mnb, 'MNB with doc2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4a9d56",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ec2a8898",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8a94dfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_pipe = Pipeline([('dt', DecisionTreeClassifier())])\n",
    "# hyperparameters selected taken from https://www.geeksforgeeks.org/how-to-tune-a-decision-tree-in-hyperparameter-tuning/\n",
    "dt_param = {'dt__max_depth': [10, 20, 30, None],'dt__min_samples_split': [2, 5, 10],'dt__min_samples_leaf': [1, 2, 4]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7d86f6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q7/lxfgdwq942qdsd3_mlwlm7p00000gn/T/ipykernel_40154/3804508306.py:6: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "===================DT with Unigrams Performance=====================\n",
      "95% CI = [ 0.6657484797758448 0.7017515202241553 ]\n",
      "Train:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.75      0.81      1593\n",
      "           1       0.78      0.89      0.83      1607\n",
      "\n",
      "    accuracy                           0.82      3200\n",
      "   macro avg       0.83      0.82      0.82      3200\n",
      "weighted avg       0.83      0.82      0.82      3200\n",
      "\n",
      "Test:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.67      0.71       407\n",
      "           1       0.69      0.76      0.72       393\n",
      "\n",
      "    accuracy                           0.71       800\n",
      "   macro avg       0.72      0.72      0.71       800\n",
      "weighted avg       0.72      0.71      0.71       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply Count Vectorizer\n",
    "X_train_cv, X_test_cv = count_vectorizer(X_train, X_test)\n",
    "\n",
    "gs3 = GridSearchCV(dt_pipe, \n",
    "                   param_grid=dt_param,\n",
    "                   cv=KFold(n_splits=5, shuffle=True, random_state=0), \n",
    "                   scoring='accuracy',\n",
    "                   verbose=1, \n",
    "                   n_jobs=-1, \n",
    "                   refit=True)\n",
    "\n",
    "gs3.fit(X_train_cv, t_train)\n",
    "dt = gs3.best_estimator_\n",
    "evaluation(X_train_cv, X_test_cv, t_train, t_test, dt, 'DT with Unigrams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4efdc669",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q7/lxfgdwq942qdsd3_mlwlm7p00000gn/T/ipykernel_40154/3804508306.py:6: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "===================DT with TF-IDF Performance=====================\n",
      "95% CI = [ 0.6648479725757676 0.7032770274242323 ]\n",
      "Train:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.87      0.91      1593\n",
      "           1       0.88      0.96      0.92      1607\n",
      "\n",
      "    accuracy                           0.92      3200\n",
      "   macro avg       0.92      0.92      0.92      3200\n",
      "weighted avg       0.92      0.92      0.92      3200\n",
      "\n",
      "Test:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.63      0.67       407\n",
      "           1       0.66      0.73      0.69       393\n",
      "\n",
      "    accuracy                           0.68       800\n",
      "   macro avg       0.68      0.68      0.68       800\n",
      "weighted avg       0.68      0.68      0.68       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply TFIDF\n",
    "X_train_tfidf, X_test_tfidf = tfidf(X_train, X_test)\n",
    "\n",
    "gs3 = GridSearchCV(dt_pipe, \n",
    "                   param_grid=dt_param,\n",
    "                   cv=KFold(n_splits=5, shuffle=True, random_state=0), \n",
    "                   scoring='accuracy',\n",
    "                   verbose=1, \n",
    "                   n_jobs=-1, \n",
    "                   refit=True)\n",
    "\n",
    "gs3.fit(X_train_tfidf, t_train)\n",
    "dt = gs3.best_estimator_\n",
    "evaluation(X_train_tfidf, X_test_tfidf, t_train, t_test, dt, 'DT with TF-IDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d6e19044",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q7/lxfgdwq942qdsd3_mlwlm7p00000gn/T/ipykernel_40154/3804508306.py:6: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "===================DT with word2vec skip-gram Performance=====================\n",
      "95% CI = [ 0.7490946438275777 0.7809053561724223 ]\n",
      "Train:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.96      0.95      1593\n",
      "           1       0.96      0.94      0.95      1607\n",
      "\n",
      "    accuracy                           0.95      3200\n",
      "   macro avg       0.95      0.95      0.95      3200\n",
      "weighted avg       0.95      0.95      0.95      3200\n",
      "\n",
      "Test:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.77      0.79       407\n",
      "           1       0.77      0.81      0.79       393\n",
      "\n",
      "    accuracy                           0.79       800\n",
      "   macro avg       0.79      0.79      0.79       800\n",
      "weighted avg       0.79      0.79      0.79       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply word2vec skipgram\n",
    "X_train_skipgram, X_test_skipgram = word2vec_skipgram(data_train, data_test)\n",
    "\n",
    "gs3 = GridSearchCV(dt_pipe, \n",
    "                   param_grid=dt_param,\n",
    "                   cv=KFold(n_splits=5, shuffle=True, random_state=0), \n",
    "                   scoring='accuracy',\n",
    "                   verbose=1, \n",
    "                   n_jobs=-1, \n",
    "                   refit=True)\n",
    "\n",
    "gs3.fit(X_train_skipgram, t_train)\n",
    "dt = gs3.best_estimator_\n",
    "evaluation(X_train_skipgram, X_test_skipgram, t_train, t_test, dt, 'DT with word2vec skip-gram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e1e634c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q7/lxfgdwq942qdsd3_mlwlm7p00000gn/T/ipykernel_40154/3804508306.py:6: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "===================DT with word2vec CBOW Performance=====================\n",
      "95% CI = [ 0.7492424934901367 0.7795075065098633 ]\n",
      "Train:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.93      0.93      1593\n",
      "           1       0.93      0.94      0.93      1607\n",
      "\n",
      "    accuracy                           0.93      3200\n",
      "   macro avg       0.93      0.93      0.93      3200\n",
      "weighted avg       0.93      0.93      0.93      3200\n",
      "\n",
      "Test:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.67      0.74       407\n",
      "           1       0.71      0.84      0.77       393\n",
      "\n",
      "    accuracy                           0.76       800\n",
      "   macro avg       0.76      0.76      0.75       800\n",
      "weighted avg       0.76      0.76      0.75       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply word2vec cbow\n",
    "X_train_cbow, X_test_cbow = word2vec_cbow(data_train, data_test)\n",
    "\n",
    "gs3 = GridSearchCV(dt_pipe, \n",
    "                   param_grid=dt_param,\n",
    "                   cv=KFold(n_splits=5, shuffle=True, random_state=0), \n",
    "                   scoring='accuracy',\n",
    "                   verbose=1, \n",
    "                   n_jobs=-1, \n",
    "                   refit=True)\n",
    "\n",
    "gs3.fit(X_train_cbow, t_train)\n",
    "dt = gs3.best_estimator_\n",
    "evaluation(X_train_cbow, X_test_cbow, t_train, t_test, dt, 'DT with word2vec CBOW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c62f0459",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q7/lxfgdwq942qdsd3_mlwlm7p00000gn/T/ipykernel_40154/3804508306.py:6: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "===================DT with Fast Text Performance=====================\n",
      "95% CI = [ 0.7107364476471649 0.7355135523528349 ]\n",
      "Train:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.90      0.86      1593\n",
      "           1       0.89      0.81      0.85      1607\n",
      "\n",
      "    accuracy                           0.85      3200\n",
      "   macro avg       0.85      0.85      0.85      3200\n",
      "weighted avg       0.85      0.85      0.85      3200\n",
      "\n",
      "Test:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.78      0.75       407\n",
      "           1       0.75      0.68      0.71       393\n",
      "\n",
      "    accuracy                           0.73       800\n",
      "   macro avg       0.73      0.73      0.73       800\n",
      "weighted avg       0.73      0.73      0.73       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply fast text\n",
    "X_train_fast, X_test_fast = fast_text(data_train, data_test)\n",
    "\n",
    "gs3 = GridSearchCV(dt_pipe, \n",
    "                   param_grid=dt_param,\n",
    "                   cv=KFold(n_splits=5, shuffle=True, random_state=0), \n",
    "                   scoring='accuracy',\n",
    "                   verbose=1, \n",
    "                   n_jobs=-1, \n",
    "                   refit=True)\n",
    "\n",
    "gs3.fit(X_train_fast, t_train)\n",
    "dt = gs3.best_estimator_\n",
    "evaluation(X_train_fast, X_test_fast, t_train, t_test, dt, 'DT with Fast Text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fe6927b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q7/lxfgdwq942qdsd3_mlwlm7p00000gn/T/ipykernel_40154/3804508306.py:6: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "===================DT with Glove Twitter 200 Performance=====================\n",
      "95% CI = [ 0.7136171852921172 0.7420078147078828 ]\n",
      "Train:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96      1593\n",
      "           1       0.97      0.95      0.96      1607\n",
      "\n",
      "    accuracy                           0.96      3200\n",
      "   macro avg       0.96      0.96      0.96      3200\n",
      "weighted avg       0.96      0.96      0.96      3200\n",
      "\n",
      "Test:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.79      0.78       407\n",
      "           1       0.77      0.75      0.76       393\n",
      "\n",
      "    accuracy                           0.77       800\n",
      "   macro avg       0.77      0.77      0.77       800\n",
      "weighted avg       0.77      0.77      0.77       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply glove\n",
    "X_train_glove, X_test_glove = glove_twitter_200(data_train, data_test)\n",
    "\n",
    "gs3 = GridSearchCV(dt_pipe, \n",
    "                   param_grid=dt_param,\n",
    "                   cv=KFold(n_splits=5, shuffle=True, random_state=0), \n",
    "                   scoring='accuracy',\n",
    "                   verbose=1, \n",
    "                   n_jobs=-1, \n",
    "                   refit=True)\n",
    "\n",
    "gs3.fit(X_train_glove, t_train)\n",
    "dt = gs3.best_estimator_\n",
    "evaluation(X_train_glove, X_test_glove, t_train, t_test, dt, 'DT with Glove Twitter 200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2610e1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.DataFrame(X_train, columns=['text']) \n",
    "data_test = pd.DataFrame(X_test, columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a77a5bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q7/lxfgdwq942qdsd3_mlwlm7p00000gn/T/ipykernel_40154/3804508306.py:6: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n",
      "/var/folders/q7/lxfgdwq942qdsd3_mlwlm7p00000gn/T/ipykernel_40154/3804508306.py:6: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "===================DT with doc2vec Performance=====================\n",
      "95% CI = [ 0.7509168289237373 0.7722081710762625 ]\n",
      "Train:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.90      0.92      1593\n",
      "           1       0.90      0.94      0.92      1607\n",
      "\n",
      "    accuracy                           0.92      3200\n",
      "   macro avg       0.92      0.92      0.92      3200\n",
      "weighted avg       0.92      0.92      0.92      3200\n",
      "\n",
      "Test:                precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.76      0.75       407\n",
      "           1       0.74      0.72      0.73       393\n",
      "\n",
      "    accuracy                           0.74       800\n",
      "   macro avg       0.74      0.74      0.74       800\n",
      "weighted avg       0.74      0.74      0.74       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Doc2Vec -- Preprocessed\n",
    "prep_data(data_train)\n",
    "prep_data(data_test)\n",
    "\n",
    "X_train_Doc2Vec, X_test_Doc2Vec = doc2vec(data_train.prep_text, data_test.prep_text)\n",
    "\n",
    "gs3 = GridSearchCV(dt_pipe, \n",
    "                   param_grid=dt_param,\n",
    "                   cv=KFold(n_splits=5, shuffle=True, random_state=0), \n",
    "                   scoring='accuracy',\n",
    "                   verbose=1, \n",
    "                   n_jobs=-1, \n",
    "                   refit=True)\n",
    "\n",
    "gs3.fit(X_train_Doc2Vec, t_train)\n",
    "dt = gs3.best_estimator_\n",
    "evaluation(X_train_Doc2Vec, X_test_Doc2Vec, t_train, t_test, dt, 'DT with doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46138ea5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
